{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hdilab/hpm/blob/master/Char-LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rR0MzkS3ewoT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "Colab = False\n",
    "NumOnBits = 10\n",
    "NumBits = 512\n",
    "Seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(2)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "vawrva3tf9qd",
    "outputId": "0bdd8846-26f2-4302-9d6d-e9edef4b674b"
   },
   "outputs": [],
   "source": [
    "if Colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    with open('/content/drive/My Drive/Colab/data/short.txt','r') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    with open('data/medium.txt','r') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZMwUVQ4aewoe",
    "outputId": "f17718fb-04ad-4cb1-b2bd-df7acbe54fac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\\n\\nThis eBook is for the use of any'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "357wL-v5ewoj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 84, 104, 101,  32,  80, 114, 111, 106, 101,  99, 116,  32,  71,\n",
       "       117, 116, 101, 110,  98, 101, 114, 103,  32,  69,  66, 111, 111,\n",
       "       107,  32, 111, 102,  32,  80, 114, 105, 100, 101,  32,  97, 110,\n",
       "       100,  32,  80, 114, 101, 106, 117, 100, 105,  99, 101,  44,  32,\n",
       "        98, 121,  32,  74,  97, 110, 101,  32,  65, 117, 115, 116, 101,\n",
       "       110,  10,  10,  84, 104, 105, 115,  32, 101,  66, 111, 111, 107,\n",
       "        32, 105, 115,  32, 102, 111, 114,  32, 116, 104, 101,  32, 117,\n",
       "       115, 101,  32, 111, 102,  32,  97, 110, 121])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_chars = [chr(i) for i in range(128)]\n",
    "chars = tuple(asc_chars)\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {c:i for i, c in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SDR class\n",
    "Handles issues with SDR\n",
    "Given a char input, generate SDR\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class SDR(object):\n",
    "    \"\"\"\n",
    "      Class implementing the SDR.\n",
    "\n",
    "      :param input_list: (List) List for input_values.\n",
    "            For ASCII it will be [chr(0), chr(1), ... chr(127)]\n",
    "\n",
    "      :param numBits: (int) Number of bits for SDR. Default value ``512``\n",
    "\n",
    "      :param numOnBits: (int) Number of Active bits for SDR. Default value ``10``.\n",
    "            It is 2% sparcity for 512 bit\n",
    "\n",
    "      :param seed: (int) Seed for the random number generator. Default value ``42``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_list,\n",
    "                 numBits=512,\n",
    "                 numOnBits=10,\n",
    "                 seed=42,\n",
    "                 inputNoise=0.1):\n",
    "\n",
    "        random.seed(seed)\n",
    "        self.population = [i for i in range(numBits)]\n",
    "        self.numOnBits = numOnBits\n",
    "        self.inputNoise = inputNoise\n",
    "        self.sdr_dict = {i:random.sample(self.population, numOnBits) for i in input_list}\n",
    "\n",
    "\n",
    "    def getSDR(self, input):\n",
    "        return self.sdr_dict[input]\n",
    "\n",
    "\n",
    "    def getNoisySDR(self, input):\n",
    "        inputSDR = self.sdr_dict[input]\n",
    "        inputSDR = [i for i in inputSDR if random.random() > self.inputNoise]\n",
    "        noise = random.sample(self.population, int(self.numOnBits * self.inputNoise))\n",
    "        return inputSDR + noise\n",
    "\n",
    "\n",
    "\n",
    "    def getInput(self, sdr):\n",
    "        \"\"\"\n",
    "        Need to implement the function which returns the corresponding input from SDR\n",
    "        This requires a probabilistic approach. Count the number of overlapping bit and nonoverlapping field.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def getCollisionProb(self, n, a, s, theta):\n",
    "        \"\"\"\n",
    "        Calculating the probability for the cases where more than theta synapses are activated\n",
    "        for different cell activation pattern\n",
    "        :param n: Number of cells\n",
    "        :param a: Number of active cells\n",
    "        :param s: Number of synapses\n",
    "        :param theta: Threshold for the dendritic activation\n",
    "        :return: The probability where dendritic activation for the different cell activation pattern\n",
    "        \"\"\"\n",
    "        numerator = 0\n",
    "        for b in range(theta, s+1):\n",
    "            numerator += combinatorial(s, b) * combinatorial(n-s, a-b)\n",
    "\n",
    "        denominator = combinatorial(n, a)\n",
    "\n",
    "        return numerator*1.0/denominator\n",
    "\n",
    "    def getRandomSDR(self):\n",
    "        noise = random.sample(self.population, numOnBits)\n",
    "        return noise\n",
    "\n",
    "\n",
    "def combinatorial(a,b):\n",
    "    return factorial(a)*1.0/factorial(a-b)/factorial(a)\n",
    "\n",
    "def factorial(a):\n",
    "    if a == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return a*factorial(a-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sdr = SDR(asc_chars,\n",
    "                numBits=NumBits,\n",
    "                numOnBits=NumOnBits,\n",
    "                seed=Seed,\n",
    "                inputNoise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDQ9Mcvjewon"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1. \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "def multi_hot_encoder(arr, n_labels):\n",
    "    multi_hot = np.zeros((arr.shape[0], arr.shape[1], n_labels), dtype=np.float32)\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            sdr = char_sdr.getNoisySDR(int2char[arr[i][j]])\n",
    "            multi_hot[i][j][np.array(sdr)] = 1  \n",
    "    return multi_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuoOhevSewo9"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr) // batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x) \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:,1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:,1:], arr[:,0] \n",
    "        yield x, y \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeUK4w9ZewpB"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 1, 3)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h42OA4H6ewpG",
    "outputId": "00fc1014-24ba-4e57-8abb-d0d19e0ea8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracySDR(output, target):\n",
    "    output, target = output.cpu(), target.cpu()\n",
    "    _ , outputIndex = output.topk(NumOnBits, dim=1)\n",
    "    _ , targetIndex = target.topk(NumOnBits, dim=1)\n",
    "    accuracy = np.zeros((outputIndex.shape[0]))\n",
    "    \n",
    "    for j in range(outputIndex.shape[0]):\n",
    "        intersection = [i for i in outputIndex[j] if i in targetIndex[j]]\n",
    "        accuracy[j] = len(intersection)*1.0/NumOnBits\n",
    "        \n",
    "    result = np.mean(accuracy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjxsXcfTewpM"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(NumBits, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, NumBits)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Flrh6R7-ewpR"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = NumBits\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = multi_hot_encoder(x, n_chars)\n",
    "            y = multi_hot_encoder(y, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length, NumBits))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            #SDR loss\n",
    "            accuracy = accuracySDR(output, targets.view(batch_size*seq_length, NumBits))\n",
    "            train.accuracy = 0.999*train.accuracy + 0.001*accuracy\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                accuacy = accuracySDR(output, targets.view(batch_size*seq_length, NumBits))\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = multi_hot_encoder(x, n_chars)\n",
    "                    y = multi_hot_encoder(y, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length, NumBits))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"SDR Acc: {:.3f}\".format(train.accuracy))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "AS5Ik3cvewpa",
    "outputId": "43630bc3-427f-46b9-f160-7ade6330efff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(512, 1024, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=1024\n",
    "n_layers=4\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HzJPMVdJewpe",
    "outputId": "135bf543-60fd-4e22-beb4-798a939d0b6a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/3000... Step: 500... Loss: 0.0798... Val Loss: 0.1250 SDR Acc: 0.084\n",
      "Epoch: 24/3000... Step: 1000... Loss: 0.0829... Val Loss: 0.1415 SDR Acc: 0.135\n",
      "Epoch: 35/3000... Step: 1500... Loss: 0.0784... Val Loss: 0.1282 SDR Acc: 0.165\n",
      "Epoch: 47/3000... Step: 2000... Loss: 0.0793... Val Loss: 0.1322 SDR Acc: 0.186\n",
      "Epoch: 59/3000... Step: 2500... Loss: 0.0750... Val Loss: 0.1353 SDR Acc: 0.199\n",
      "Epoch: 70/3000... Step: 3000... Loss: 0.0783... Val Loss: 0.1392 SDR Acc: 0.205\n",
      "Epoch: 82/3000... Step: 3500... Loss: 0.0805... Val Loss: 0.1289 SDR Acc: 0.211\n",
      "Epoch: 94/3000... Step: 4000... Loss: 0.0232... Val Loss: 0.1471 SDR Acc: 0.214\n",
      "Epoch: 105/3000... Step: 4500... Loss: 0.0779... Val Loss: 0.1507 SDR Acc: 0.217\n",
      "Epoch: 117/3000... Step: 5000... Loss: 0.0750... Val Loss: 0.1479 SDR Acc: 0.220\n",
      "Epoch: 128/3000... Step: 5500... Loss: 0.0785... Val Loss: 0.1488 SDR Acc: 0.220\n",
      "Epoch: 140/3000... Step: 6000... Loss: 0.0778... Val Loss: 0.1532 SDR Acc: 0.223\n",
      "Epoch: 152/3000... Step: 6500... Loss: 0.0787... Val Loss: 0.1510 SDR Acc: 0.226\n",
      "Epoch: 163/3000... Step: 7000... Loss: 0.0764... Val Loss: 0.1609 SDR Acc: 0.225\n",
      "Epoch: 175/3000... Step: 7500... Loss: 0.0828... Val Loss: 0.1393 SDR Acc: 0.228\n",
      "Epoch: 187/3000... Step: 8000... Loss: 0.0231... Val Loss: 0.1613 SDR Acc: 0.229\n",
      "Epoch: 198/3000... Step: 8500... Loss: 0.0790... Val Loss: 0.1622 SDR Acc: 0.230\n",
      "Epoch: 210/3000... Step: 9000... Loss: 0.0773... Val Loss: 0.1547 SDR Acc: 0.232\n",
      "Epoch: 221/3000... Step: 9500... Loss: 0.0807... Val Loss: 0.1578 SDR Acc: 0.232\n",
      "Epoch: 233/3000... Step: 10000... Loss: 0.0799... Val Loss: 0.1628 SDR Acc: 0.235\n",
      "Epoch: 245/3000... Step: 10500... Loss: 0.0775... Val Loss: 0.1623 SDR Acc: 0.238\n",
      "Epoch: 256/3000... Step: 11000... Loss: 0.0780... Val Loss: 0.1657 SDR Acc: 0.238\n",
      "Epoch: 268/3000... Step: 11500... Loss: 0.0777... Val Loss: 0.1640 SDR Acc: 0.241\n",
      "Epoch: 280/3000... Step: 12000... Loss: 0.0228... Val Loss: 0.1657 SDR Acc: 0.242\n",
      "Epoch: 291/3000... Step: 12500... Loss: 0.0772... Val Loss: 0.1660 SDR Acc: 0.243\n",
      "Epoch: 303/3000... Step: 13000... Loss: 0.0766... Val Loss: 0.1587 SDR Acc: 0.245\n",
      "Epoch: 314/3000... Step: 13500... Loss: 0.0804... Val Loss: 0.1617 SDR Acc: 0.244\n",
      "Epoch: 326/3000... Step: 14000... Loss: 0.0772... Val Loss: 0.1406 SDR Acc: 0.247\n",
      "Epoch: 338/3000... Step: 14500... Loss: 0.0752... Val Loss: 0.1429 SDR Acc: 0.250\n",
      "Epoch: 349/3000... Step: 15000... Loss: 0.0774... Val Loss: 0.1543 SDR Acc: 0.248\n",
      "Epoch: 361/3000... Step: 15500... Loss: 0.0770... Val Loss: 0.1636 SDR Acc: 0.252\n",
      "Epoch: 373/3000... Step: 16000... Loss: 0.0269... Val Loss: 0.1595 SDR Acc: 0.254\n",
      "Epoch: 384/3000... Step: 16500... Loss: 0.0791... Val Loss: 0.1429 SDR Acc: 0.253\n",
      "Epoch: 396/3000... Step: 17000... Loss: 0.0793... Val Loss: 0.1654 SDR Acc: 0.256\n",
      "Epoch: 407/3000... Step: 17500... Loss: 0.0802... Val Loss: 0.1619 SDR Acc: 0.255\n",
      "Epoch: 419/3000... Step: 18000... Loss: 0.0771... Val Loss: 0.1610 SDR Acc: 0.258\n",
      "Epoch: 431/3000... Step: 18500... Loss: 0.0751... Val Loss: 0.1624 SDR Acc: 0.261\n",
      "Epoch: 442/3000... Step: 19000... Loss: 0.0795... Val Loss: 0.1591 SDR Acc: 0.259\n",
      "Epoch: 454/3000... Step: 19500... Loss: 0.0802... Val Loss: 0.1655 SDR Acc: 0.262\n",
      "Epoch: 466/3000... Step: 20000... Loss: 0.0301... Val Loss: 0.1428 SDR Acc: 0.264\n",
      "Epoch: 477/3000... Step: 20500... Loss: 0.0780... Val Loss: 0.1634 SDR Acc: 0.264\n",
      "Epoch: 489/3000... Step: 21000... Loss: 0.0727... Val Loss: 0.1648 SDR Acc: 0.266\n",
      "Epoch: 500/3000... Step: 21500... Loss: 0.0779... Val Loss: 0.1618 SDR Acc: 0.265\n",
      "Epoch: 512/3000... Step: 22000... Loss: 0.0795... Val Loss: 0.1608 SDR Acc: 0.267\n",
      "Epoch: 524/3000... Step: 22500... Loss: 0.0786... Val Loss: 0.1609 SDR Acc: 0.269\n",
      "Epoch: 535/3000... Step: 23000... Loss: 0.0778... Val Loss: 0.1649 SDR Acc: 0.268\n",
      "Epoch: 547/3000... Step: 23500... Loss: 0.0822... Val Loss: 0.1430 SDR Acc: 0.271\n",
      "Epoch: 559/3000... Step: 24000... Loss: 0.0368... Val Loss: 0.1637 SDR Acc: 0.274\n",
      "Epoch: 570/3000... Step: 24500... Loss: 0.0800... Val Loss: 0.1659 SDR Acc: 0.273\n",
      "Epoch: 582/3000... Step: 25000... Loss: 0.0782... Val Loss: 0.1594 SDR Acc: 0.276\n",
      "Epoch: 594/3000... Step: 25500... Loss: 0.0229... Val Loss: 0.1624 SDR Acc: 0.276\n",
      "Epoch: 605/3000... Step: 26000... Loss: 0.0794... Val Loss: 0.1628 SDR Acc: 0.278\n",
      "Epoch: 617/3000... Step: 26500... Loss: 0.0743... Val Loss: 0.1656 SDR Acc: 0.283\n",
      "Epoch: 628/3000... Step: 27000... Loss: 0.0775... Val Loss: 0.1645 SDR Acc: 0.281\n",
      "Epoch: 640/3000... Step: 27500... Loss: 0.0781... Val Loss: 0.1621 SDR Acc: 0.286\n",
      "Epoch: 652/3000... Step: 28000... Loss: 0.0387... Val Loss: 0.1398 SDR Acc: 0.290\n",
      "Epoch: 663/3000... Step: 28500... Loss: 0.0760... Val Loss: 0.1674 SDR Acc: 0.291\n",
      "Epoch: 675/3000... Step: 29000... Loss: 0.0810... Val Loss: 0.1588 SDR Acc: 0.295\n",
      "Epoch: 687/3000... Step: 29500... Loss: 0.0232... Val Loss: 0.1514 SDR Acc: 0.297\n",
      "Epoch: 698/3000... Step: 30000... Loss: 0.0791... Val Loss: 0.1646 SDR Acc: 0.299\n",
      "Epoch: 710/3000... Step: 30500... Loss: 0.0750... Val Loss: 0.1659 SDR Acc: 0.302\n",
      "Epoch: 721/3000... Step: 31000... Loss: 0.0790... Val Loss: 0.1628 SDR Acc: 0.301\n",
      "Epoch: 733/3000... Step: 31500... Loss: 0.0756... Val Loss: 0.1644 SDR Acc: 0.307\n",
      "Epoch: 745/3000... Step: 32000... Loss: 0.0508... Val Loss: 0.1597 SDR Acc: 0.309\n",
      "Epoch: 756/3000... Step: 32500... Loss: 0.0809... Val Loss: 0.1630 SDR Acc: 0.308\n",
      "Epoch: 768/3000... Step: 33000... Loss: 0.0762... Val Loss: 0.1646 SDR Acc: 0.312\n",
      "Epoch: 780/3000... Step: 33500... Loss: 0.0238... Val Loss: 0.1640 SDR Acc: 0.312\n",
      "Epoch: 791/3000... Step: 34000... Loss: 0.0767... Val Loss: 0.1554 SDR Acc: 0.314\n",
      "Epoch: 803/3000... Step: 34500... Loss: 0.0759... Val Loss: 0.1389 SDR Acc: 0.318\n",
      "Epoch: 814/3000... Step: 35000... Loss: 0.0803... Val Loss: 0.1620 SDR Acc: 0.316\n",
      "Epoch: 826/3000... Step: 35500... Loss: 0.0787... Val Loss: 0.1652 SDR Acc: 0.320\n",
      "Epoch: 838/3000... Step: 36000... Loss: 0.0353... Val Loss: 0.1636 SDR Acc: 0.323\n",
      "Epoch: 849/3000... Step: 36500... Loss: 0.0772... Val Loss: 0.1625 SDR Acc: 0.322\n",
      "Epoch: 861/3000... Step: 37000... Loss: 0.0752... Val Loss: 0.1676 SDR Acc: 0.328\n",
      "Epoch: 873/3000... Step: 37500... Loss: 0.0229... Val Loss: 0.1548 SDR Acc: 0.328\n",
      "Epoch: 884/3000... Step: 38000... Loss: 0.0805... Val Loss: 0.1621 SDR Acc: 0.330\n",
      "Epoch: 896/3000... Step: 38500... Loss: 0.0760... Val Loss: 0.1624 SDR Acc: 0.335\n",
      "Epoch: 907/3000... Step: 39000... Loss: 0.0794... Val Loss: 0.1656 SDR Acc: 0.333\n",
      "Epoch: 919/3000... Step: 39500... Loss: 0.0759... Val Loss: 0.1678 SDR Acc: 0.339\n",
      "Epoch: 931/3000... Step: 40000... Loss: 0.0379... Val Loss: 0.1696 SDR Acc: 0.342\n",
      "Epoch: 942/3000... Step: 40500... Loss: 0.0775... Val Loss: 0.1622 SDR Acc: 0.341\n",
      "Epoch: 954/3000... Step: 41000... Loss: 0.0771... Val Loss: 0.1460 SDR Acc: 0.348\n",
      "Epoch: 966/3000... Step: 41500... Loss: 0.0231... Val Loss: 0.1632 SDR Acc: 0.348\n",
      "Epoch: 977/3000... Step: 42000... Loss: 0.0766... Val Loss: 0.1585 SDR Acc: 0.349\n",
      "Epoch: 989/3000... Step: 42500... Loss: 0.0750... Val Loss: 0.1650 SDR Acc: 0.355\n",
      "Epoch: 1000/3000... Step: 43000... Loss: 0.0771... Val Loss: 0.1612 SDR Acc: 0.353\n",
      "Epoch: 1012/3000... Step: 43500... Loss: 0.0785... Val Loss: 0.1651 SDR Acc: 0.359\n",
      "Epoch: 1024/3000... Step: 44000... Loss: 0.0389... Val Loss: 0.1673 SDR Acc: 0.363\n",
      "Epoch: 1035/3000... Step: 44500... Loss: 0.0801... Val Loss: 0.1664 SDR Acc: 0.362\n",
      "Epoch: 1047/3000... Step: 45000... Loss: 0.0795... Val Loss: 0.1570 SDR Acc: 0.368\n",
      "Epoch: 1059/3000... Step: 45500... Loss: 0.0229... Val Loss: 0.1639 SDR Acc: 0.368\n",
      "Epoch: 1070/3000... Step: 46000... Loss: 0.0785... Val Loss: 0.1411 SDR Acc: 0.365\n",
      "Epoch: 1082/3000... Step: 46500... Loss: 0.0815... Val Loss: 0.1618 SDR Acc: 0.374\n",
      "Epoch: 1094/3000... Step: 47000... Loss: 0.0239... Val Loss: 0.1627 SDR Acc: 0.374\n",
      "Epoch: 1105/3000... Step: 47500... Loss: 0.0800... Val Loss: 0.1618 SDR Acc: 0.380\n",
      "Epoch: 1117/3000... Step: 48000... Loss: 0.0367... Val Loss: 0.1659 SDR Acc: 0.388\n",
      "Epoch: 1128/3000... Step: 48500... Loss: 0.0777... Val Loss: 0.1641 SDR Acc: 0.388\n",
      "Epoch: 1140/3000... Step: 49000... Loss: 0.0771... Val Loss: 0.1469 SDR Acc: 0.396\n",
      "Epoch: 1152/3000... Step: 49500... Loss: 0.0232... Val Loss: 0.1670 SDR Acc: 0.394\n",
      "Epoch: 1163/3000... Step: 50000... Loss: 0.0772... Val Loss: 0.1632 SDR Acc: 0.398\n",
      "Epoch: 1175/3000... Step: 50500... Loss: 0.0657... Val Loss: 0.1646 SDR Acc: 0.407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1187/3000... Step: 51000... Loss: 0.0231... Val Loss: 0.1617 SDR Acc: 0.405\n",
      "Epoch: 1198/3000... Step: 51500... Loss: 0.0798... Val Loss: 0.1600 SDR Acc: 0.408\n",
      "Epoch: 1210/3000... Step: 52000... Loss: 0.0367... Val Loss: 0.1641 SDR Acc: 0.405\n",
      "Epoch: 1221/3000... Step: 52500... Loss: 0.0812... Val Loss: 0.1605 SDR Acc: 0.410\n",
      "Epoch: 1233/3000... Step: 53000... Loss: 0.0760... Val Loss: 0.1660 SDR Acc: 0.418\n",
      "Epoch: 1245/3000... Step: 53500... Loss: 0.0229... Val Loss: 0.1611 SDR Acc: 0.420\n",
      "Epoch: 1256/3000... Step: 54000... Loss: 0.0784... Val Loss: 0.1602 SDR Acc: 0.420\n",
      "Epoch: 1268/3000... Step: 54500... Loss: 0.0474... Val Loss: 0.1523 SDR Acc: 0.433\n",
      "Epoch: 1280/3000... Step: 55000... Loss: 0.0225... Val Loss: 0.1556 SDR Acc: 0.419\n",
      "Epoch: 1291/3000... Step: 55500... Loss: 0.0754... Val Loss: 0.1631 SDR Acc: 0.426\n",
      "Epoch: 1303/3000... Step: 56000... Loss: 0.0646... Val Loss: 0.1644 SDR Acc: 0.434\n",
      "Epoch: 1314/3000... Step: 56500... Loss: 0.0810... Val Loss: 0.1598 SDR Acc: 0.431\n",
      "Epoch: 1326/3000... Step: 57000... Loss: 0.0790... Val Loss: 0.1522 SDR Acc: 0.447\n",
      "Epoch: 1338/3000... Step: 57500... Loss: 0.0245... Val Loss: 0.1622 SDR Acc: 0.452\n",
      "Epoch: 1349/3000... Step: 58000... Loss: 0.0807... Val Loss: 0.1646 SDR Acc: 0.433\n",
      "Epoch: 1361/3000... Step: 58500... Loss: 0.0550... Val Loss: 0.1524 SDR Acc: 0.433\n",
      "Epoch: 1373/3000... Step: 59000... Loss: 0.0229... Val Loss: 0.1663 SDR Acc: 0.438\n",
      "Epoch: 1384/3000... Step: 59500... Loss: 0.0778... Val Loss: 0.1581 SDR Acc: 0.453\n",
      "Epoch: 1396/3000... Step: 60000... Loss: 0.0516... Val Loss: 0.1627 SDR Acc: 0.465\n",
      "Epoch: 1407/3000... Step: 60500... Loss: 0.0814... Val Loss: 0.1545 SDR Acc: 0.468\n",
      "Epoch: 1419/3000... Step: 61000... Loss: 0.0758... Val Loss: 0.1554 SDR Acc: 0.475\n",
      "Epoch: 1431/3000... Step: 61500... Loss: 0.0268... Val Loss: 0.1637 SDR Acc: 0.469\n",
      "Epoch: 1442/3000... Step: 62000... Loss: 0.0793... Val Loss: 0.1459 SDR Acc: 0.468\n",
      "Epoch: 1454/3000... Step: 62500... Loss: 0.0423... Val Loss: 0.1530 SDR Acc: 0.485\n",
      "Epoch: 1466/3000... Step: 63000... Loss: 0.0231... Val Loss: 0.1586 SDR Acc: 0.494\n",
      "Epoch: 1477/3000... Step: 63500... Loss: 0.0768... Val Loss: 0.1589 SDR Acc: 0.505\n",
      "Epoch: 1489/3000... Step: 64000... Loss: 0.0296... Val Loss: 0.1649 SDR Acc: 0.499\n",
      "Epoch: 1500/3000... Step: 64500... Loss: 0.0770... Val Loss: 0.1601 SDR Acc: 0.508\n",
      "Epoch: 1512/3000... Step: 65000... Loss: 0.0696... Val Loss: 0.1522 SDR Acc: 0.523\n",
      "Epoch: 1524/3000... Step: 65500... Loss: 0.0270... Val Loss: 0.1619 SDR Acc: 0.498\n",
      "Epoch: 1535/3000... Step: 66000... Loss: 0.0777... Val Loss: 0.1623 SDR Acc: 0.500\n",
      "Epoch: 1547/3000... Step: 66500... Loss: 0.0377... Val Loss: 0.1475 SDR Acc: 0.521\n",
      "Epoch: 1559/3000... Step: 67000... Loss: 0.0233... Val Loss: 0.1459 SDR Acc: 0.532\n",
      "Epoch: 1570/3000... Step: 67500... Loss: 0.0782... Val Loss: 0.1470 SDR Acc: 0.531\n",
      "Epoch: 1582/3000... Step: 68000... Loss: 0.0505... Val Loss: 0.1416 SDR Acc: 0.542\n",
      "Epoch: 1594/3000... Step: 68500... Loss: 0.0224... Val Loss: 0.1619 SDR Acc: 0.547\n",
      "Epoch: 1605/3000... Step: 69000... Loss: 0.0671... Val Loss: 0.1475 SDR Acc: 0.555\n",
      "Epoch: 1617/3000... Step: 69500... Loss: 0.0228... Val Loss: 0.1524 SDR Acc: 0.555\n",
      "Epoch: 1628/3000... Step: 70000... Loss: 0.0775... Val Loss: 0.1591 SDR Acc: 0.557\n",
      "Epoch: 1640/3000... Step: 70500... Loss: 0.0453... Val Loss: 0.1536 SDR Acc: 0.558\n",
      "Epoch: 1652/3000... Step: 71000... Loss: 0.0257... Val Loss: 0.1661 SDR Acc: 0.567\n",
      "Epoch: 1663/3000... Step: 71500... Loss: 0.0738... Val Loss: 0.1593 SDR Acc: 0.555\n",
      "Epoch: 1675/3000... Step: 72000... Loss: 0.0305... Val Loss: 0.1563 SDR Acc: 0.569\n",
      "Epoch: 1687/3000... Step: 72500... Loss: 0.0220... Val Loss: 0.1451 SDR Acc: 0.559\n",
      "Epoch: 1698/3000... Step: 73000... Loss: 0.0402... Val Loss: 0.1524 SDR Acc: 0.586\n",
      "Epoch: 1710/3000... Step: 73500... Loss: 0.0267... Val Loss: 0.1534 SDR Acc: 0.596\n",
      "Epoch: 1721/3000... Step: 74000... Loss: 0.0812... Val Loss: 0.1658 SDR Acc: 0.589\n",
      "Epoch: 1733/3000... Step: 74500... Loss: 0.0380... Val Loss: 0.1527 SDR Acc: 0.605\n",
      "Epoch: 1745/3000... Step: 75000... Loss: 0.0222... Val Loss: 0.1586 SDR Acc: 0.602\n",
      "Epoch: 1756/3000... Step: 75500... Loss: 0.0761... Val Loss: 0.1618 SDR Acc: 0.613\n",
      "Epoch: 1768/3000... Step: 76000... Loss: 0.0401... Val Loss: 0.1623 SDR Acc: 0.591\n",
      "Epoch: 1780/3000... Step: 76500... Loss: 0.0228... Val Loss: 0.1594 SDR Acc: 0.568\n",
      "Epoch: 1791/3000... Step: 77000... Loss: 0.0423... Val Loss: 0.1463 SDR Acc: 0.589\n",
      "Epoch: 1803/3000... Step: 77500... Loss: 0.0372... Val Loss: 0.1576 SDR Acc: 0.587\n",
      "Epoch: 1814/3000... Step: 78000... Loss: 0.0784... Val Loss: 0.1563 SDR Acc: 0.579\n",
      "Epoch: 1826/3000... Step: 78500... Loss: 0.0498... Val Loss: 0.1548 SDR Acc: 0.573\n",
      "Epoch: 1838/3000... Step: 79000... Loss: 0.0228... Val Loss: 0.1628 SDR Acc: 0.574\n",
      "Epoch: 1849/3000... Step: 79500... Loss: 0.0605... Val Loss: 0.1634 SDR Acc: 0.601\n",
      "Epoch: 1861/3000... Step: 80000... Loss: 0.0407... Val Loss: 0.1585 SDR Acc: 0.608\n",
      "Epoch: 1873/3000... Step: 80500... Loss: 0.0228... Val Loss: 0.1531 SDR Acc: 0.617\n",
      "Epoch: 1884/3000... Step: 81000... Loss: 0.0541... Val Loss: 0.1642 SDR Acc: 0.608\n",
      "Epoch: 1896/3000... Step: 81500... Loss: 0.0252... Val Loss: 0.1422 SDR Acc: 0.635\n",
      "Epoch: 1907/3000... Step: 82000... Loss: 0.0811... Val Loss: 0.1622 SDR Acc: 0.617\n",
      "Epoch: 1919/3000... Step: 82500... Loss: 0.0371... Val Loss: 0.1358 SDR Acc: 0.641\n",
      "Epoch: 1931/3000... Step: 83000... Loss: 0.0260... Val Loss: 0.1618 SDR Acc: 0.634\n",
      "Epoch: 1942/3000... Step: 83500... Loss: 0.0450... Val Loss: 0.1570 SDR Acc: 0.658\n",
      "Epoch: 1954/3000... Step: 84000... Loss: 0.0457... Val Loss: 0.1646 SDR Acc: 0.630\n",
      "Epoch: 1966/3000... Step: 84500... Loss: 0.0236... Val Loss: 0.1489 SDR Acc: 0.638\n",
      "Epoch: 1977/3000... Step: 85000... Loss: 0.0373... Val Loss: 0.1551 SDR Acc: 0.664\n",
      "Epoch: 1989/3000... Step: 85500... Loss: 0.0379... Val Loss: 0.1609 SDR Acc: 0.605\n",
      "Epoch: 2000/3000... Step: 86000... Loss: 0.0772... Val Loss: 0.1543 SDR Acc: 0.617\n",
      "Epoch: 2012/3000... Step: 86500... Loss: 0.0387... Val Loss: 0.1607 SDR Acc: 0.652\n",
      "Epoch: 2024/3000... Step: 87000... Loss: 0.0243... Val Loss: 0.1597 SDR Acc: 0.672\n",
      "Epoch: 2035/3000... Step: 87500... Loss: 0.0407... Val Loss: 0.1639 SDR Acc: 0.687\n",
      "Epoch: 2047/3000... Step: 88000... Loss: 0.0900... Val Loss: 0.1606 SDR Acc: 0.659\n",
      "Epoch: 2059/3000... Step: 88500... Loss: 0.0241... Val Loss: 0.1655 SDR Acc: 0.603\n",
      "Epoch: 2070/3000... Step: 89000... Loss: 0.0794... Val Loss: 0.1572 SDR Acc: 0.581\n",
      "Epoch: 2082/3000... Step: 89500... Loss: 0.0368... Val Loss: 0.1473 SDR Acc: 0.595\n",
      "Epoch: 2094/3000... Step: 90000... Loss: 0.0217... Val Loss: 0.1562 SDR Acc: 0.646\n",
      "Epoch: 2105/3000... Step: 90500... Loss: 0.0389... Val Loss: 0.1424 SDR Acc: 0.647\n",
      "Epoch: 2117/3000... Step: 91000... Loss: 0.0239... Val Loss: 0.1596 SDR Acc: 0.669\n",
      "Epoch: 2128/3000... Step: 91500... Loss: 0.0450... Val Loss: 0.1623 SDR Acc: 0.694\n",
      "Epoch: 2140/3000... Step: 92000... Loss: 0.0345... Val Loss: 0.1451 SDR Acc: 0.703\n",
      "Epoch: 2152/3000... Step: 92500... Loss: 0.0245... Val Loss: 0.1537 SDR Acc: 0.703\n",
      "Epoch: 2163/3000... Step: 93000... Loss: 0.0771... Val Loss: 0.1378 SDR Acc: 0.635\n",
      "Epoch: 2175/3000... Step: 93500... Loss: 0.0350... Val Loss: 0.1655 SDR Acc: 0.561\n",
      "Epoch: 2187/3000... Step: 94000... Loss: 0.0224... Val Loss: 0.1686 SDR Acc: 0.588\n",
      "Epoch: 2198/3000... Step: 94500... Loss: 0.0411... Val Loss: 0.1659 SDR Acc: 0.648\n",
      "Epoch: 2210/3000... Step: 95000... Loss: 0.0237... Val Loss: 0.1626 SDR Acc: 0.688\n",
      "Epoch: 2221/3000... Step: 95500... Loss: 0.0471... Val Loss: 0.1531 SDR Acc: 0.714\n",
      "Epoch: 2233/3000... Step: 96000... Loss: 0.0319... Val Loss: 0.1584 SDR Acc: 0.706\n",
      "Epoch: 2245/3000... Step: 96500... Loss: 0.0225... Val Loss: 0.1652 SDR Acc: 0.716\n",
      "Epoch: 2256/3000... Step: 97000... Loss: 0.0370... Val Loss: 0.1514 SDR Acc: 0.728\n",
      "Epoch: 2268/3000... Step: 97500... Loss: 0.0530... Val Loss: 0.1651 SDR Acc: 0.693\n",
      "Epoch: 2280/3000... Step: 98000... Loss: 0.0238... Val Loss: 0.1579 SDR Acc: 0.630\n",
      "Epoch: 2291/3000... Step: 98500... Loss: 0.0371... Val Loss: 0.1653 SDR Acc: 0.636\n",
      "Epoch: 2303/3000... Step: 99000... Loss: 0.0225... Val Loss: 0.1661 SDR Acc: 0.688\n",
      "Epoch: 2314/3000... Step: 99500... Loss: 0.0547... Val Loss: 0.1576 SDR Acc: 0.701\n",
      "Epoch: 2326/3000... Step: 100000... Loss: 0.0516... Val Loss: 0.1649 SDR Acc: 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2338/3000... Step: 100500... Loss: 0.0248... Val Loss: 0.1558 SDR Acc: 0.662\n",
      "Epoch: 2349/3000... Step: 101000... Loss: 0.0368... Val Loss: 0.1592 SDR Acc: 0.699\n",
      "Epoch: 2361/3000... Step: 101500... Loss: 0.0418... Val Loss: 0.1596 SDR Acc: 0.723\n",
      "Epoch: 2373/3000... Step: 102000... Loss: 0.0237... Val Loss: 0.1613 SDR Acc: 0.725\n",
      "Epoch: 2384/3000... Step: 102500... Loss: 0.0358... Val Loss: 0.1664 SDR Acc: 0.745\n",
      "Epoch: 2396/3000... Step: 103000... Loss: 0.0227... Val Loss: 0.1502 SDR Acc: 0.732\n",
      "Epoch: 2407/3000... Step: 103500... Loss: 0.0380... Val Loss: 0.1615 SDR Acc: 0.746\n",
      "Epoch: 2419/3000... Step: 104000... Loss: 0.0319... Val Loss: 0.1678 SDR Acc: 0.758\n",
      "Epoch: 2431/3000... Step: 104500... Loss: 0.0238... Val Loss: 0.1557 SDR Acc: 0.759\n",
      "Epoch: 2442/3000... Step: 105000... Loss: 0.0664... Val Loss: 0.1548 SDR Acc: 0.751\n",
      "Epoch: 2454/3000... Step: 105500... Loss: 0.0251... Val Loss: 0.1678 SDR Acc: 0.743\n",
      "Epoch: 2466/3000... Step: 106000... Loss: 0.0236... Val Loss: 0.1611 SDR Acc: 0.751\n",
      "Epoch: 2477/3000... Step: 106500... Loss: 0.0375... Val Loss: 0.1597 SDR Acc: 0.740\n",
      "Epoch: 2489/3000... Step: 107000... Loss: 0.0316... Val Loss: 0.1648 SDR Acc: 0.753\n",
      "Epoch: 2500/3000... Step: 107500... Loss: 0.0396... Val Loss: 0.1643 SDR Acc: 0.764\n",
      "Epoch: 2512/3000... Step: 108000... Loss: 0.0798... Val Loss: 0.1595 SDR Acc: 0.679\n",
      "Epoch: 2524/3000... Step: 108500... Loss: 0.0226... Val Loss: 0.1660 SDR Acc: 0.627\n",
      "Epoch: 2535/3000... Step: 109000... Loss: 0.0386... Val Loss: 0.1647 SDR Acc: 0.673\n",
      "Epoch: 2547/3000... Step: 109500... Loss: 0.0232... Val Loss: 0.1648 SDR Acc: 0.724\n",
      "Epoch: 2559/3000... Step: 110000... Loss: 0.0224... Val Loss: 0.1622 SDR Acc: 0.721\n",
      "Epoch: 2570/3000... Step: 110500... Loss: 0.0365... Val Loss: 0.1581 SDR Acc: 0.737\n",
      "Epoch: 2582/3000... Step: 111000... Loss: 0.0242... Val Loss: 0.1606 SDR Acc: 0.747\n",
      "Epoch: 2594/3000... Step: 111500... Loss: 0.0228... Val Loss: 0.1519 SDR Acc: 0.738\n",
      "Epoch: 2605/3000... Step: 112000... Loss: 0.0678... Val Loss: 0.1625 SDR Acc: 0.620\n",
      "Epoch: 2617/3000... Step: 112500... Loss: 0.0228... Val Loss: 0.1623 SDR Acc: 0.612\n",
      "Epoch: 2628/3000... Step: 113000... Loss: 0.0362... Val Loss: 0.1640 SDR Acc: 0.675\n",
      "Epoch: 2640/3000... Step: 113500... Loss: 0.0241... Val Loss: 0.1687 SDR Acc: 0.726\n",
      "Epoch: 2652/3000... Step: 114000... Loss: 0.0233... Val Loss: 0.1604 SDR Acc: 0.751\n",
      "Epoch: 2663/3000... Step: 114500... Loss: 0.0359... Val Loss: 0.1572 SDR Acc: 0.764\n",
      "Epoch: 2675/3000... Step: 115000... Loss: 0.0261... Val Loss: 0.1533 SDR Acc: 0.768\n",
      "Epoch: 2687/3000... Step: 115500... Loss: 0.0216... Val Loss: 0.1494 SDR Acc: 0.781\n",
      "Epoch: 2698/3000... Step: 116000... Loss: 0.0395... Val Loss: 0.1631 SDR Acc: 0.710\n",
      "Epoch: 2710/3000... Step: 116500... Loss: 0.0221... Val Loss: 0.1602 SDR Acc: 0.741\n",
      "Epoch: 2721/3000... Step: 117000... Loss: 0.0367... Val Loss: 0.1594 SDR Acc: 0.752\n",
      "Epoch: 2733/3000... Step: 117500... Loss: 0.0397... Val Loss: 0.1643 SDR Acc: 0.758\n",
      "Epoch: 2745/3000... Step: 118000... Loss: 0.0235... Val Loss: 0.1579 SDR Acc: 0.662\n",
      "Epoch: 2756/3000... Step: 118500... Loss: 0.0383... Val Loss: 0.1665 SDR Acc: 0.684\n",
      "Epoch: 2768/3000... Step: 119000... Loss: 0.0255... Val Loss: 0.1573 SDR Acc: 0.719\n",
      "Epoch: 2780/3000... Step: 119500... Loss: 0.0232... Val Loss: 0.1659 SDR Acc: 0.741\n",
      "Epoch: 2791/3000... Step: 120000... Loss: 0.0503... Val Loss: 0.1506 SDR Acc: 0.747\n",
      "Epoch: 2803/3000... Step: 120500... Loss: 0.0247... Val Loss: 0.1643 SDR Acc: 0.739\n",
      "Epoch: 2814/3000... Step: 121000... Loss: 0.0384... Val Loss: 0.1592 SDR Acc: 0.655\n",
      "Epoch: 2826/3000... Step: 121500... Loss: 0.0258... Val Loss: 0.1524 SDR Acc: 0.717\n",
      "Epoch: 2838/3000... Step: 122000... Loss: 0.0230... Val Loss: 0.1389 SDR Acc: 0.757\n",
      "Epoch: 2849/3000... Step: 122500... Loss: 0.0337... Val Loss: 0.1494 SDR Acc: 0.774\n",
      "Epoch: 2861/3000... Step: 123000... Loss: 0.0684... Val Loss: 0.1450 SDR Acc: 0.773\n",
      "Epoch: 2873/3000... Step: 123500... Loss: 0.0219... Val Loss: 0.1664 SDR Acc: 0.703\n",
      "Epoch: 2884/3000... Step: 124000... Loss: 0.0460... Val Loss: 0.1622 SDR Acc: 0.694\n",
      "Epoch: 2896/3000... Step: 124500... Loss: 0.0225... Val Loss: 0.1490 SDR Acc: 0.723\n",
      "Epoch: 2907/3000... Step: 125000... Loss: 0.0361... Val Loss: 0.1711 SDR Acc: 0.759\n",
      "Epoch: 2919/3000... Step: 125500... Loss: 0.0241... Val Loss: 0.1497 SDR Acc: 0.782\n",
      "Epoch: 2931/3000... Step: 126000... Loss: 0.0222... Val Loss: 0.1493 SDR Acc: 0.765\n",
      "Epoch: 2942/3000... Step: 126500... Loss: 0.0667... Val Loss: 0.1569 SDR Acc: 0.765\n",
      "Epoch: 2954/3000... Step: 127000... Loss: 0.0420... Val Loss: 0.1578 SDR Acc: 0.726\n",
      "Epoch: 2966/3000... Step: 127500... Loss: 0.0220... Val Loss: 0.1566 SDR Acc: 0.740\n",
      "Epoch: 2977/3000... Step: 128000... Loss: 0.0539... Val Loss: 0.1611 SDR Acc: 0.759\n",
      "Epoch: 2989/3000... Step: 128500... Loss: 0.0218... Val Loss: 0.1609 SDR Acc: 0.769\n",
      "Epoch: 3000/3000... Step: 129000... Loss: 0.0379... Val Loss: 0.1614 SDR Acc: 0.769\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "seq_length = 10 #max length verses\n",
    "n_epochs = 3000 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train.accuracy = 0 \n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.0001, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 2, 16)\n",
    "x, y = next(batches)\n",
    "\n",
    "x = multi_hot_encoder(x, NumBits)\n",
    "y = multi_hot_encoder(y, NumBits)\n",
    "\n",
    "inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "if(train_on_gpu):\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    \n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "output, h = net(inputs, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = targets.view(batch_size*seq_length, NumBits)\n",
    "# a = a.cpu()\n",
    "# print(np.argwhere(a>0))\n",
    "b = output.cpu()\n",
    "values, indices = b.topk(NumOnBits, dim=1)\n",
    "print(indices.shape)\n",
    "print(indices)\n",
    "print(np.argwhere(b>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbSGCJO4ewpi"
   },
   "outputs": [],
   "source": [
    "model_dante = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_dante, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65NVMnmmewpm"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encoder(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        # apply softmax to get p probabilities for the likely next character giving x\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        # considering the k most probable characters with topk method\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cQz9pJnewps"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='Il', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "ko1gyKMIewpv",
    "outputId": "2c1356c1-fde5-4932-f3b3-128f73459aaa"
   },
   "outputs": [],
   "source": [
    "print(sample(net, 1000, prime='This ', top_k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH5K12reewpy",
    "outputId": "98ba7d8e-f290-407b-86eb-490264eb4cbb"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23n3y1TOewp1",
    "outputId": "5dbde076-cb3b-4593-c3a4-c2e52c22f279"
   },
   "outputs": [],
   "source": [
    "x, y = next(batches)\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7MjK0kEewp5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Char-LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
