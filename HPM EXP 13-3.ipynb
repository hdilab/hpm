{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hdilab/hpm/blob/master/Char-LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rR0MzkS3ewoT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "Colab = False\n",
    "NumOnBits = 10\n",
    "NumBits = 512\n",
    "Seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(1)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "vawrva3tf9qd",
    "outputId": "0bdd8846-26f2-4302-9d6d-e9edef4b674b"
   },
   "outputs": [],
   "source": [
    "if Colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    with open('/content/drive/My Drive/Colab/data/short.txt','r') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    with open('data/short.txt','r') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZMwUVQ4aewoe",
    "outputId": "f17718fb-04ad-4cb1-b2bd-df7acbe54fac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Space is big. You just won't believe how vastly, hugely, mind-bogglingly big it is. I mean, you may \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "357wL-v5ewoj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 83, 112,  97,  99, 101,  32, 105, 115,  32,  98, 105, 103,  46,\n",
       "        32,  89, 111, 117,  32, 106, 117, 115, 116,  32, 119, 111, 110,\n",
       "        39, 116,  32,  98, 101, 108, 105, 101, 118, 101,  32, 104, 111,\n",
       "       119,  32, 118,  97, 115, 116, 108, 121,  44,  32, 104, 117, 103,\n",
       "       101, 108, 121,  44,  32, 109, 105, 110, 100,  45,  98, 111, 103,\n",
       "       103, 108, 105, 110, 103, 108, 121,  32,  98, 105, 103,  32, 105,\n",
       "       116,  32, 105, 115,  46,  32,  73,  32, 109, 101,  97, 110,  44,\n",
       "        32, 121, 111, 117,  32, 109,  97, 121,  32])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_chars = [chr(i) for i in range(128)]\n",
    "chars = tuple(asc_chars)\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {c:i for i, c in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SDR class\n",
    "Handles issues with SDR\n",
    "Given a char input, generate SDR\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class SDR(object):\n",
    "    \"\"\"\n",
    "      Class implementing the SDR.\n",
    "\n",
    "      :param input_list: (List) List for input_values.\n",
    "            For ASCII it will be [chr(0), chr(1), ... chr(127)]\n",
    "\n",
    "      :param numBits: (int) Number of bits for SDR. Default value ``512``\n",
    "\n",
    "      :param numOnBits: (int) Number of Active bits for SDR. Default value ``10``.\n",
    "            It is 2% sparcity for 512 bit\n",
    "\n",
    "      :param seed: (int) Seed for the random number generator. Default value ``42``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_list,\n",
    "                 numBits=512,\n",
    "                 numOnBits=10,\n",
    "                 seed=42,\n",
    "                 inputNoise=0.1):\n",
    "\n",
    "        random.seed(seed)\n",
    "        self.population = [i for i in range(numBits)]\n",
    "        self.numOnBits = numOnBits\n",
    "        self.inputNoise = inputNoise\n",
    "        self.sdr_dict = {i:random.sample(self.population, numOnBits) for i in input_list}\n",
    "\n",
    "\n",
    "    def getSDR(self, input):\n",
    "        return self.sdr_dict[input]\n",
    "\n",
    "\n",
    "    def getNoisySDR(self, input):\n",
    "        inputSDR = self.sdr_dict[input]\n",
    "        inputSDR = [i for i in inputSDR if random.random() > self.inputNoise]\n",
    "        noise = random.sample(self.population, int(self.numOnBits * self.inputNoise))\n",
    "        return inputSDR + noise\n",
    "\n",
    "\n",
    "\n",
    "    def getInput(self, sdr):\n",
    "        \"\"\"\n",
    "        Need to implement the function which returns the corresponding input from SDR\n",
    "        This requires a probabilistic approach. Count the number of overlapping bit and nonoverlapping field.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def getCollisionProb(self, n, a, s, theta):\n",
    "        \"\"\"\n",
    "        Calculating the probability for the cases where more than theta synapses are activated\n",
    "        for different cell activation pattern\n",
    "        :param n: Number of cells\n",
    "        :param a: Number of active cells\n",
    "        :param s: Number of synapses\n",
    "        :param theta: Threshold for the dendritic activation\n",
    "        :return: The probability where dendritic activation for the different cell activation pattern\n",
    "        \"\"\"\n",
    "        numerator = 0\n",
    "        for b in range(theta, s+1):\n",
    "            numerator += combinatorial(s, b) * combinatorial(n-s, a-b)\n",
    "\n",
    "        denominator = combinatorial(n, a)\n",
    "\n",
    "        return numerator*1.0/denominator\n",
    "\n",
    "    def getRandomSDR(self):\n",
    "        noise = random.sample(self.population, numOnBits)\n",
    "        return noise\n",
    "\n",
    "\n",
    "def combinatorial(a,b):\n",
    "    return factorial(a)*1.0/factorial(a-b)/factorial(a)\n",
    "\n",
    "def factorial(a):\n",
    "    if a == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return a*factorial(a-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sdr = SDR(asc_chars,\n",
    "                numBits=NumBits,\n",
    "                numOnBits=NumOnBits,\n",
    "                seed=Seed,\n",
    "                inputNoise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDQ9Mcvjewon"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1. \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "def multi_hot_encoder(arr, n_labels):\n",
    "    multi_hot = np.zeros((arr.shape[0], arr.shape[1], n_labels), dtype=np.float32)\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            sdr = char_sdr.getNoisySDR(int2char[arr[i][j]])\n",
    "            multi_hot[i][j][np.array(sdr)] = 1  \n",
    "    return multi_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuoOhevSewo9"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr) // batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x) \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:,1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:,1:], arr[:,0] \n",
    "        yield x, y \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeUK4w9ZewpB"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 1, 3)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h42OA4H6ewpG",
    "outputId": "00fc1014-24ba-4e57-8abb-d0d19e0ea8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracySDR(output, target):\n",
    "    output, target = output.cpu(), target.cpu()\n",
    "    _ , outputIndex = output.topk(NumOnBits, dim=1)\n",
    "    _ , targetIndex = target.topk(NumOnBits, dim=1)\n",
    "    accuracy = np.zeros((outputIndex.shape[0]))\n",
    "    \n",
    "    for j in range(outputIndex.shape[0]):\n",
    "        intersection = [i for i in outputIndex[j] if i in targetIndex[j]]\n",
    "        accuracy[j] = len(intersection)*1.0/NumOnBits\n",
    "        \n",
    "    result = np.mean(accuracy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjxsXcfTewpM"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(NumBits, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, NumBits)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Flrh6R7-ewpR"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = NumBits\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = multi_hot_encoder(x, n_chars)\n",
    "            y = multi_hot_encoder(y, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length, NumBits))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            #SDR loss\n",
    "            accuracy = accuracySDR(output, targets.view(batch_size*seq_length, NumBits))\n",
    "            train.accuracy = 0.9999*train.accuracy + 0.0001*accuracy\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                accuacy = accuracySDR(output, targets.view(batch_size*seq_length, NumBits))\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = multi_hot_encoder(x, n_chars)\n",
    "                    y = multi_hot_encoder(y, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length, NumBits))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"SDR Acc: {:.3f}\".format(train.accuracy))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "AS5Ik3cvewpa",
    "outputId": "43630bc3-427f-46b9-f160-7ade6330efff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(512, 1024, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=1024\n",
    "n_layers=4\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HzJPMVdJewpe",
    "outputId": "135bf543-60fd-4e22-beb4-798a939d0b6a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/3000... Step: 300... Loss: 0.0784... Val Loss: 0.0794 SDR Acc: 0.005\n",
      "Epoch: 38/3000... Step: 600... Loss: 0.0736... Val Loss: 0.0744 SDR Acc: 0.010\n",
      "Epoch: 57/3000... Step: 900... Loss: 0.0795... Val Loss: 0.0759 SDR Acc: 0.015\n",
      "Epoch: 75/3000... Step: 1200... Loss: 0.0818... Val Loss: 0.0776 SDR Acc: 0.020\n",
      "Epoch: 94/3000... Step: 1500... Loss: 0.0745... Val Loss: 0.0765 SDR Acc: 0.025\n",
      "Epoch: 113/3000... Step: 1800... Loss: 0.0696... Val Loss: 0.0721 SDR Acc: 0.029\n",
      "Epoch: 132/3000... Step: 2100... Loss: 0.0787... Val Loss: 0.0729 SDR Acc: 0.034\n",
      "Epoch: 150/3000... Step: 2400... Loss: 0.0778... Val Loss: 0.0763 SDR Acc: 0.039\n",
      "Epoch: 169/3000... Step: 2700... Loss: 0.0770... Val Loss: 0.0741 SDR Acc: 0.043\n",
      "Epoch: 188/3000... Step: 3000... Loss: 0.0717... Val Loss: 0.0781 SDR Acc: 0.047\n",
      "Epoch: 207/3000... Step: 3300... Loss: 0.0754... Val Loss: 0.0757 SDR Acc: 0.052\n",
      "Epoch: 225/3000... Step: 3600... Loss: 0.0799... Val Loss: 0.0797 SDR Acc: 0.056\n",
      "Epoch: 244/3000... Step: 3900... Loss: 0.0790... Val Loss: 0.0769 SDR Acc: 0.060\n",
      "Epoch: 263/3000... Step: 4200... Loss: 0.0688... Val Loss: 0.0795 SDR Acc: 0.064\n",
      "Epoch: 282/3000... Step: 4500... Loss: 0.0759... Val Loss: 0.0821 SDR Acc: 0.068\n",
      "Epoch: 300/3000... Step: 4800... Loss: 0.0772... Val Loss: 0.0841 SDR Acc: 0.072\n",
      "Epoch: 319/3000... Step: 5100... Loss: 0.0757... Val Loss: 0.0823 SDR Acc: 0.076\n",
      "Epoch: 338/3000... Step: 5400... Loss: 0.0693... Val Loss: 0.0825 SDR Acc: 0.079\n",
      "Epoch: 357/3000... Step: 5700... Loss: 0.0740... Val Loss: 0.0846 SDR Acc: 0.083\n",
      "Epoch: 375/3000... Step: 6000... Loss: 0.0792... Val Loss: 0.0861 SDR Acc: 0.086\n",
      "Epoch: 394/3000... Step: 6300... Loss: 0.0786... Val Loss: 0.0846 SDR Acc: 0.089\n",
      "Epoch: 413/3000... Step: 6600... Loss: 0.0708... Val Loss: 0.0833 SDR Acc: 0.093\n",
      "Epoch: 432/3000... Step: 6900... Loss: 0.0778... Val Loss: 0.0856 SDR Acc: 0.096\n",
      "Epoch: 450/3000... Step: 7200... Loss: 0.0778... Val Loss: 0.0890 SDR Acc: 0.099\n",
      "Epoch: 469/3000... Step: 7500... Loss: 0.0725... Val Loss: 0.0900 SDR Acc: 0.102\n",
      "Epoch: 488/3000... Step: 7800... Loss: 0.0716... Val Loss: 0.0936 SDR Acc: 0.105\n",
      "Epoch: 507/3000... Step: 8100... Loss: 0.0734... Val Loss: 0.0890 SDR Acc: 0.107\n",
      "Epoch: 525/3000... Step: 8400... Loss: 0.0755... Val Loss: 0.0915 SDR Acc: 0.110\n",
      "Epoch: 544/3000... Step: 8700... Loss: 0.0786... Val Loss: 0.0935 SDR Acc: 0.113\n",
      "Epoch: 563/3000... Step: 9000... Loss: 0.0704... Val Loss: 0.0968 SDR Acc: 0.116\n",
      "Epoch: 582/3000... Step: 9300... Loss: 0.0771... Val Loss: 0.0964 SDR Acc: 0.118\n",
      "Epoch: 600/3000... Step: 9600... Loss: 0.0808... Val Loss: 0.0983 SDR Acc: 0.121\n",
      "Epoch: 619/3000... Step: 9900... Loss: 0.0812... Val Loss: 0.0930 SDR Acc: 0.123\n",
      "Epoch: 638/3000... Step: 10200... Loss: 0.0723... Val Loss: 0.0966 SDR Acc: 0.126\n",
      "Epoch: 657/3000... Step: 10500... Loss: 0.0778... Val Loss: 0.0967 SDR Acc: 0.128\n",
      "Epoch: 675/3000... Step: 10800... Loss: 0.0746... Val Loss: 0.0950 SDR Acc: 0.130\n",
      "Epoch: 694/3000... Step: 11100... Loss: 0.0788... Val Loss: 0.1004 SDR Acc: 0.133\n",
      "Epoch: 713/3000... Step: 11400... Loss: 0.0720... Val Loss: 0.0993 SDR Acc: 0.135\n",
      "Epoch: 732/3000... Step: 11700... Loss: 0.0730... Val Loss: 0.0980 SDR Acc: 0.138\n",
      "Epoch: 750/3000... Step: 12000... Loss: 0.0816... Val Loss: 0.0958 SDR Acc: 0.140\n",
      "Epoch: 769/3000... Step: 12300... Loss: 0.0786... Val Loss: 0.0973 SDR Acc: 0.142\n",
      "Epoch: 788/3000... Step: 12600... Loss: 0.0676... Val Loss: 0.1009 SDR Acc: 0.145\n",
      "Epoch: 807/3000... Step: 12900... Loss: 0.0742... Val Loss: 0.1015 SDR Acc: 0.147\n",
      "Epoch: 825/3000... Step: 13200... Loss: 0.0799... Val Loss: 0.1027 SDR Acc: 0.150\n",
      "Epoch: 844/3000... Step: 13500... Loss: 0.0783... Val Loss: 0.1049 SDR Acc: 0.153\n",
      "Epoch: 863/3000... Step: 13800... Loss: 0.0672... Val Loss: 0.1122 SDR Acc: 0.156\n",
      "Epoch: 882/3000... Step: 14100... Loss: 0.0737... Val Loss: 0.1119 SDR Acc: 0.159\n",
      "Epoch: 900/3000... Step: 14400... Loss: 0.0784... Val Loss: 0.1230 SDR Acc: 0.162\n",
      "Epoch: 919/3000... Step: 14700... Loss: 0.0805... Val Loss: 0.1187 SDR Acc: 0.166\n",
      "Epoch: 938/3000... Step: 15000... Loss: 0.0701... Val Loss: 0.1071 SDR Acc: 0.167\n",
      "Epoch: 957/3000... Step: 15300... Loss: 0.0729... Val Loss: 0.1090 SDR Acc: 0.170\n",
      "Epoch: 975/3000... Step: 15600... Loss: 0.0697... Val Loss: 0.1081 SDR Acc: 0.173\n",
      "Epoch: 994/3000... Step: 15900... Loss: 0.0703... Val Loss: 0.1042 SDR Acc: 0.178\n",
      "Epoch: 1013/3000... Step: 16200... Loss: 0.0566... Val Loss: 0.0939 SDR Acc: 0.184\n",
      "Epoch: 1032/3000... Step: 16500... Loss: 0.0572... Val Loss: 0.1365 SDR Acc: 0.191\n",
      "Epoch: 1050/3000... Step: 16800... Loss: 0.0576... Val Loss: 0.1058 SDR Acc: 0.199\n",
      "Epoch: 1069/3000... Step: 17100... Loss: 0.0604... Val Loss: 0.1481 SDR Acc: 0.208\n",
      "Epoch: 1088/3000... Step: 17400... Loss: 0.0432... Val Loss: 0.1091 SDR Acc: 0.219\n",
      "Epoch: 1107/3000... Step: 17700... Loss: 0.0418... Val Loss: 0.1586 SDR Acc: 0.232\n",
      "Epoch: 1125/3000... Step: 18000... Loss: 0.0388... Val Loss: 0.1587 SDR Acc: 0.246\n",
      "Epoch: 1144/3000... Step: 18300... Loss: 0.0423... Val Loss: 0.1619 SDR Acc: 0.260\n",
      "Epoch: 1163/3000... Step: 18600... Loss: 0.0346... Val Loss: 0.1667 SDR Acc: 0.276\n",
      "Epoch: 1182/3000... Step: 18900... Loss: 0.0322... Val Loss: 0.1451 SDR Acc: 0.292\n",
      "Epoch: 1200/3000... Step: 19200... Loss: 0.0398... Val Loss: 0.1571 SDR Acc: 0.308\n",
      "Epoch: 1219/3000... Step: 19500... Loss: 0.0265... Val Loss: 0.1187 SDR Acc: 0.323\n",
      "Epoch: 1238/3000... Step: 19800... Loss: 0.0284... Val Loss: 0.1705 SDR Acc: 0.339\n",
      "Epoch: 1257/3000... Step: 20100... Loss: 0.0276... Val Loss: 0.1242 SDR Acc: 0.355\n",
      "Epoch: 1275/3000... Step: 20400... Loss: 0.0250... Val Loss: 0.1245 SDR Acc: 0.370\n",
      "Epoch: 1294/3000... Step: 20700... Loss: 0.0268... Val Loss: 0.1662 SDR Acc: 0.384\n",
      "Epoch: 1313/3000... Step: 21000... Loss: 0.0217... Val Loss: 0.1600 SDR Acc: 0.398\n",
      "Epoch: 1332/3000... Step: 21300... Loss: 0.0245... Val Loss: 0.1348 SDR Acc: 0.412\n",
      "Epoch: 1350/3000... Step: 21600... Loss: 0.0231... Val Loss: 0.1597 SDR Acc: 0.425\n",
      "Epoch: 1369/3000... Step: 21900... Loss: 0.0255... Val Loss: 0.1695 SDR Acc: 0.438\n",
      "Epoch: 1388/3000... Step: 22200... Loss: 0.0259... Val Loss: 0.1715 SDR Acc: 0.451\n",
      "Epoch: 1407/3000... Step: 22500... Loss: 0.0252... Val Loss: 0.1762 SDR Acc: 0.463\n",
      "Epoch: 1425/3000... Step: 22800... Loss: 0.0204... Val Loss: 0.1535 SDR Acc: 0.475\n",
      "Epoch: 1444/3000... Step: 23100... Loss: 0.0203... Val Loss: 0.1596 SDR Acc: 0.487\n",
      "Epoch: 1463/3000... Step: 23400... Loss: 0.0202... Val Loss: 0.1187 SDR Acc: 0.498\n",
      "Epoch: 1482/3000... Step: 23700... Loss: 0.0269... Val Loss: 0.1800 SDR Acc: 0.509\n",
      "Epoch: 1500/3000... Step: 24000... Loss: 0.0232... Val Loss: 0.1638 SDR Acc: 0.519\n",
      "Epoch: 1519/3000... Step: 24300... Loss: 0.0235... Val Loss: 0.1402 SDR Acc: 0.530\n",
      "Epoch: 1538/3000... Step: 24600... Loss: 0.0219... Val Loss: 0.1624 SDR Acc: 0.540\n",
      "Epoch: 1557/3000... Step: 24900... Loss: 0.0229... Val Loss: 0.1575 SDR Acc: 0.549\n",
      "Epoch: 1575/3000... Step: 25200... Loss: 0.0215... Val Loss: 0.1596 SDR Acc: 0.559\n",
      "Epoch: 1594/3000... Step: 25500... Loss: 0.0216... Val Loss: 0.1641 SDR Acc: 0.568\n",
      "Epoch: 1613/3000... Step: 25800... Loss: 0.0246... Val Loss: 0.1704 SDR Acc: 0.577\n",
      "Epoch: 1632/3000... Step: 26100... Loss: 0.0233... Val Loss: 0.1537 SDR Acc: 0.585\n",
      "Epoch: 1650/3000... Step: 26400... Loss: 0.0241... Val Loss: 0.1605 SDR Acc: 0.594\n",
      "Epoch: 1669/3000... Step: 26700... Loss: 0.0231... Val Loss: 0.1675 SDR Acc: 0.602\n",
      "Epoch: 1688/3000... Step: 27000... Loss: 0.0241... Val Loss: 0.1553 SDR Acc: 0.610\n",
      "Epoch: 1707/3000... Step: 27300... Loss: 0.0219... Val Loss: 0.1611 SDR Acc: 0.617\n",
      "Epoch: 1725/3000... Step: 27600... Loss: 0.0233... Val Loss: 0.1634 SDR Acc: 0.625\n",
      "Epoch: 1744/3000... Step: 27900... Loss: 0.0225... Val Loss: 0.1557 SDR Acc: 0.632\n",
      "Epoch: 1763/3000... Step: 28200... Loss: 0.0208... Val Loss: 0.1599 SDR Acc: 0.639\n",
      "Epoch: 1782/3000... Step: 28500... Loss: 0.0225... Val Loss: 0.1504 SDR Acc: 0.646\n",
      "Epoch: 1800/3000... Step: 28800... Loss: 0.0250... Val Loss: 0.1716 SDR Acc: 0.652\n",
      "Epoch: 1819/3000... Step: 29100... Loss: 0.0210... Val Loss: 0.1730 SDR Acc: 0.659\n",
      "Epoch: 1838/3000... Step: 29400... Loss: 0.0249... Val Loss: 0.1545 SDR Acc: 0.665\n",
      "Epoch: 1857/3000... Step: 29700... Loss: 0.0210... Val Loss: 0.1629 SDR Acc: 0.671\n",
      "Epoch: 1875/3000... Step: 30000... Loss: 0.0217... Val Loss: 0.1658 SDR Acc: 0.677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1894/3000... Step: 30300... Loss: 0.0195... Val Loss: 0.1712 SDR Acc: 0.682\n",
      "Epoch: 1913/3000... Step: 30600... Loss: 0.0200... Val Loss: 0.1621 SDR Acc: 0.688\n",
      "Epoch: 1932/3000... Step: 30900... Loss: 0.0228... Val Loss: 0.1709 SDR Acc: 0.693\n",
      "Epoch: 1950/3000... Step: 31200... Loss: 0.0234... Val Loss: 0.1594 SDR Acc: 0.699\n",
      "Epoch: 1969/3000... Step: 31500... Loss: 0.0198... Val Loss: 0.1644 SDR Acc: 0.704\n",
      "Epoch: 1988/3000... Step: 31800... Loss: 0.0237... Val Loss: 0.1668 SDR Acc: 0.708\n",
      "Epoch: 2007/3000... Step: 32100... Loss: 0.0215... Val Loss: 0.1578 SDR Acc: 0.713\n",
      "Epoch: 2025/3000... Step: 32400... Loss: 0.0223... Val Loss: 0.1618 SDR Acc: 0.718\n",
      "Epoch: 2044/3000... Step: 32700... Loss: 0.0184... Val Loss: 0.1617 SDR Acc: 0.722\n",
      "Epoch: 2063/3000... Step: 33000... Loss: 0.0236... Val Loss: 0.1713 SDR Acc: 0.726\n",
      "Epoch: 2082/3000... Step: 33300... Loss: 0.0205... Val Loss: 0.1817 SDR Acc: 0.731\n",
      "Epoch: 2100/3000... Step: 33600... Loss: 0.0216... Val Loss: 0.1836 SDR Acc: 0.735\n",
      "Epoch: 2119/3000... Step: 33900... Loss: 0.0202... Val Loss: 0.1779 SDR Acc: 0.739\n",
      "Epoch: 2138/3000... Step: 34200... Loss: 0.0188... Val Loss: 0.1618 SDR Acc: 0.743\n",
      "Epoch: 2157/3000... Step: 34500... Loss: 0.0238... Val Loss: 0.1636 SDR Acc: 0.746\n",
      "Epoch: 2175/3000... Step: 34800... Loss: 0.0208... Val Loss: 0.1593 SDR Acc: 0.750\n",
      "Epoch: 2194/3000... Step: 35100... Loss: 0.0198... Val Loss: 0.1642 SDR Acc: 0.754\n",
      "Epoch: 2213/3000... Step: 35400... Loss: 0.0220... Val Loss: 0.1628 SDR Acc: 0.757\n",
      "Epoch: 2232/3000... Step: 35700... Loss: 0.0204... Val Loss: 0.1688 SDR Acc: 0.760\n",
      "Epoch: 2250/3000... Step: 36000... Loss: 0.0235... Val Loss: 0.1673 SDR Acc: 0.763\n",
      "Epoch: 2269/3000... Step: 36300... Loss: 0.0196... Val Loss: 0.1594 SDR Acc: 0.767\n",
      "Epoch: 2288/3000... Step: 36600... Loss: 0.0221... Val Loss: 0.1552 SDR Acc: 0.770\n",
      "Epoch: 2307/3000... Step: 36900... Loss: 0.0200... Val Loss: 0.1719 SDR Acc: 0.773\n",
      "Epoch: 2325/3000... Step: 37200... Loss: 0.0203... Val Loss: 0.1644 SDR Acc: 0.776\n",
      "Epoch: 2344/3000... Step: 37500... Loss: 0.0208... Val Loss: 0.1694 SDR Acc: 0.778\n",
      "Epoch: 2363/3000... Step: 37800... Loss: 0.0217... Val Loss: 0.1661 SDR Acc: 0.781\n",
      "Epoch: 2382/3000... Step: 38100... Loss: 0.0185... Val Loss: 0.1697 SDR Acc: 0.784\n",
      "Epoch: 2400/3000... Step: 38400... Loss: 0.0208... Val Loss: 0.1708 SDR Acc: 0.786\n",
      "Epoch: 2419/3000... Step: 38700... Loss: 0.0205... Val Loss: 0.1680 SDR Acc: 0.789\n",
      "Epoch: 2438/3000... Step: 39000... Loss: 0.0193... Val Loss: 0.1790 SDR Acc: 0.791\n",
      "Epoch: 2457/3000... Step: 39300... Loss: 0.0198... Val Loss: 0.1730 SDR Acc: 0.793\n",
      "Epoch: 2475/3000... Step: 39600... Loss: 0.0206... Val Loss: 0.1706 SDR Acc: 0.796\n",
      "Epoch: 2494/3000... Step: 39900... Loss: 0.0201... Val Loss: 0.1739 SDR Acc: 0.798\n",
      "Epoch: 2513/3000... Step: 40200... Loss: 0.0208... Val Loss: 0.1740 SDR Acc: 0.800\n",
      "Epoch: 2532/3000... Step: 40500... Loss: 0.0225... Val Loss: 0.1757 SDR Acc: 0.802\n",
      "Epoch: 2550/3000... Step: 40800... Loss: 0.0239... Val Loss: 0.1639 SDR Acc: 0.804\n",
      "Epoch: 2569/3000... Step: 41100... Loss: 0.0229... Val Loss: 0.1700 SDR Acc: 0.806\n",
      "Epoch: 2588/3000... Step: 41400... Loss: 0.0201... Val Loss: 0.1716 SDR Acc: 0.808\n",
      "Epoch: 2607/3000... Step: 41700... Loss: 0.0220... Val Loss: 0.1780 SDR Acc: 0.809\n",
      "Epoch: 2625/3000... Step: 42000... Loss: 0.0192... Val Loss: 0.1699 SDR Acc: 0.811\n",
      "Epoch: 2644/3000... Step: 42300... Loss: 0.0230... Val Loss: 0.1759 SDR Acc: 0.813\n",
      "Epoch: 2663/3000... Step: 42600... Loss: 0.0204... Val Loss: 0.1754 SDR Acc: 0.815\n",
      "Epoch: 2682/3000... Step: 42900... Loss: 0.0216... Val Loss: 0.1683 SDR Acc: 0.816\n",
      "Epoch: 2700/3000... Step: 43200... Loss: 0.0237... Val Loss: 0.1803 SDR Acc: 0.818\n",
      "Epoch: 2719/3000... Step: 43500... Loss: 0.0225... Val Loss: 0.1757 SDR Acc: 0.819\n",
      "Epoch: 2738/3000... Step: 43800... Loss: 0.0206... Val Loss: 0.1750 SDR Acc: 0.821\n",
      "Epoch: 2757/3000... Step: 44100... Loss: 0.0218... Val Loss: 0.1820 SDR Acc: 0.822\n",
      "Epoch: 2775/3000... Step: 44400... Loss: 0.0199... Val Loss: 0.1770 SDR Acc: 0.824\n",
      "Epoch: 2794/3000... Step: 44700... Loss: 0.0206... Val Loss: 0.1797 SDR Acc: 0.825\n",
      "Epoch: 2813/3000... Step: 45000... Loss: 0.0230... Val Loss: 0.1791 SDR Acc: 0.826\n",
      "Epoch: 2832/3000... Step: 45300... Loss: 0.0213... Val Loss: 0.1803 SDR Acc: 0.827\n",
      "Epoch: 2850/3000... Step: 45600... Loss: 0.0213... Val Loss: 0.1759 SDR Acc: 0.829\n",
      "Epoch: 2869/3000... Step: 45900... Loss: 0.0185... Val Loss: 0.1789 SDR Acc: 0.830\n",
      "Epoch: 2888/3000... Step: 46200... Loss: 0.0213... Val Loss: 0.1785 SDR Acc: 0.831\n",
      "Epoch: 2907/3000... Step: 46500... Loss: 0.0223... Val Loss: 0.1771 SDR Acc: 0.832\n",
      "Epoch: 2925/3000... Step: 46800... Loss: 0.0219... Val Loss: 0.1814 SDR Acc: 0.833\n",
      "Epoch: 2944/3000... Step: 47100... Loss: 0.0219... Val Loss: 0.1784 SDR Acc: 0.834\n",
      "Epoch: 2963/3000... Step: 47400... Loss: 0.0221... Val Loss: 0.1797 SDR Acc: 0.835\n",
      "Epoch: 2982/3000... Step: 47700... Loss: 0.0215... Val Loss: 0.1680 SDR Acc: 0.836\n",
      "Epoch: 3000/3000... Step: 48000... Loss: 0.0204... Val Loss: 0.1747 SDR Acc: 0.837\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_length = 10 #max length verses\n",
    "n_epochs = 3000 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train.accuracy = 0 \n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.0001, print_every=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 2, 16)\n",
    "x, y = next(batches)\n",
    "\n",
    "x = multi_hot_encoder(x, NumBits)\n",
    "y = multi_hot_encoder(y, NumBits)\n",
    "\n",
    "inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "if(train_on_gpu):\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    \n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "output, h = net(inputs, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = targets.view(batch_size*seq_length, NumBits)\n",
    "# a = a.cpu()\n",
    "# print(np.argwhere(a>0))\n",
    "b = output.cpu()\n",
    "values, indices = b.topk(NumOnBits, dim=1)\n",
    "print(indices.shape)\n",
    "print(indices)\n",
    "print(np.argwhere(b>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbSGCJO4ewpi"
   },
   "outputs": [],
   "source": [
    "model_dante = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_dante, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65NVMnmmewpm"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encoder(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        # apply softmax to get p probabilities for the likely next character giving x\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        # considering the k most probable characters with topk method\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cQz9pJnewps"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='Il', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "ko1gyKMIewpv",
    "outputId": "2c1356c1-fde5-4932-f3b3-128f73459aaa"
   },
   "outputs": [],
   "source": [
    "print(sample(net, 1000, prime='This ', top_k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH5K12reewpy",
    "outputId": "98ba7d8e-f290-407b-86eb-490264eb4cbb"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23n3y1TOewp1",
    "outputId": "5dbde076-cb3b-4593-c3a4-c2e52c22f279"
   },
   "outputs": [],
   "source": [
    "x, y = next(batches)\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7MjK0kEewp5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Char-LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
