{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hdilab/hpm/blob/master/Char-LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rR0MzkS3ewoT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "Colab = False\n",
    "NumOnBits = 10\n",
    "NumBits = 512\n",
    "Seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "vawrva3tf9qd",
    "outputId": "0bdd8846-26f2-4302-9d6d-e9edef4b674b"
   },
   "outputs": [],
   "source": [
    "if Colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    with open('/content/drive/My Drive/Colab/data/short.txt','r') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    with open('data/1342.txt','r') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZMwUVQ4aewoe",
    "outputId": "f17718fb-04ad-4cb1-b2bd-df7acbe54fac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\\n\\nThis eBook is for the use of any'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "357wL-v5ewoj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 84, 104, 101,  32,  80, 114, 111, 106, 101,  99, 116,  32,  71,\n",
       "       117, 116, 101, 110,  98, 101, 114, 103,  32,  69,  66, 111, 111,\n",
       "       107,  32, 111, 102,  32,  80, 114, 105, 100, 101,  32,  97, 110,\n",
       "       100,  32,  80, 114, 101, 106, 117, 100, 105,  99, 101,  44,  32,\n",
       "        98, 121,  32,  74,  97, 110, 101,  32,  65, 117, 115, 116, 101,\n",
       "       110,  10,  10,  84, 104, 105, 115,  32, 101,  66, 111, 111, 107,\n",
       "        32, 105, 115,  32, 102, 111, 114,  32, 116, 104, 101,  32, 117,\n",
       "       115, 101,  32, 111, 102,  32,  97, 110, 121])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_chars = [chr(i) for i in range(128)]\n",
    "chars = tuple(asc_chars)\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {c:i for i, c in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SDR class\n",
    "Handles issues with SDR\n",
    "Given a char input, generate SDR\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class SDR(object):\n",
    "    \"\"\"\n",
    "      Class implementing the SDR.\n",
    "\n",
    "      :param input_list: (List) List for input_values.\n",
    "            For ASCII it will be [chr(0), chr(1), ... chr(127)]\n",
    "\n",
    "      :param numBits: (int) Number of bits for SDR. Default value ``512``\n",
    "\n",
    "      :param numOnBits: (int) Number of Active bits for SDR. Default value ``10``.\n",
    "            It is 2% sparcity for 512 bit\n",
    "\n",
    "      :param seed: (int) Seed for the random number generator. Default value ``42``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_list,\n",
    "                 numBits=512,\n",
    "                 numOnBits=10,\n",
    "                 seed=42,\n",
    "                 inputNoise=0.1):\n",
    "\n",
    "        random.seed(seed)\n",
    "        self.population = [i for i in range(numBits)]\n",
    "        self.numOnBits = numOnBits\n",
    "        self.inputNoise = inputNoise\n",
    "        self.sdr_dict = {i:random.sample(self.population, numOnBits) for i in input_list}\n",
    "\n",
    "\n",
    "    def getSDR(self, input):\n",
    "        return self.sdr_dict[input]\n",
    "\n",
    "\n",
    "    def getNoisySDR(self, input):\n",
    "        inputSDR = self.sdr_dict[input]\n",
    "        inputSDR = [i for i in inputSDR if random.random() > self.inputNoise]\n",
    "        noise = random.sample(self.population, int(self.numOnBits * self.inputNoise))\n",
    "        return inputSDR + noise\n",
    "\n",
    "\n",
    "\n",
    "    def getInput(self, sdr):\n",
    "        \"\"\"\n",
    "        Need to implement the function which returns the corresponding input from SDR\n",
    "        This requires a probabilistic approach. Count the number of overlapping bit and nonoverlapping field.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def getCollisionProb(self, n, a, s, theta):\n",
    "        \"\"\"\n",
    "        Calculating the probability for the cases where more than theta synapses are activated\n",
    "        for different cell activation pattern\n",
    "        :param n: Number of cells\n",
    "        :param a: Number of active cells\n",
    "        :param s: Number of synapses\n",
    "        :param theta: Threshold for the dendritic activation\n",
    "        :return: The probability where dendritic activation for the different cell activation pattern\n",
    "        \"\"\"\n",
    "        numerator = 0\n",
    "        for b in range(theta, s+1):\n",
    "            numerator += combinatorial(s, b) * combinatorial(n-s, a-b)\n",
    "\n",
    "        denominator = combinatorial(n, a)\n",
    "\n",
    "        return numerator*1.0/denominator\n",
    "\n",
    "    def getRandomSDR(self):\n",
    "        noise = random.sample(self.population, numOnBits)\n",
    "        return noise\n",
    "\n",
    "\n",
    "def combinatorial(a,b):\n",
    "    return factorial(a)*1.0/factorial(a-b)/factorial(a)\n",
    "\n",
    "def factorial(a):\n",
    "    if a == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return a*factorial(a-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sdr = SDR(asc_chars,\n",
    "                numBits=NumBits,\n",
    "                numOnBits=NumOnBits,\n",
    "                seed=Seed,\n",
    "                inputNoise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDQ9Mcvjewon"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1. \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "def multi_hot_encoder(arr, n_labels):\n",
    "    multi_hot = np.zeros((arr.shape[0], arr.shape[1], n_labels), dtype=np.float32)\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            sdr = char_sdr.getNoisySDR(int2char[arr[i][j]])\n",
    "            multi_hot[i][j][np.array(sdr)] = 1  \n",
    "    return multi_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuoOhevSewo9"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr) // batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x) \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:,1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:,1:], arr[:,0] \n",
    "        yield x, y \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeUK4w9ZewpB"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h42OA4H6ewpG",
    "outputId": "00fc1014-24ba-4e57-8abb-d0d19e0ea8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracySDR(output, target):\n",
    "    output, target = output.cpu(), target.cpu()\n",
    "    _ , outputIndex = output.topk(NumOnBits, dim=1)\n",
    "    _ , targetIndex = target.topk(NumOnBits, dim=1)\n",
    "    accuracy = np.zeros((outputIndex.shape[0]))\n",
    "    \n",
    "    for j in range(outputIndex.shape[0]):\n",
    "        intersection = [i for i in outputIndex[j] if i in targetIndex[j]]\n",
    "        accuracy[j] = len(intersection)*1.0/NumOnBits\n",
    "        \n",
    "    result = np.mean(accuracy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjxsXcfTewpM"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(NumBits, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, NumBits)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Flrh6R7-ewpR"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = NumBits\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = multi_hot_encoder(x, n_chars)\n",
    "            y = multi_hot_encoder(y, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length, NumBits))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            #SDR loss\n",
    "            accuracy = accuracySDR(output, targets.view(batch_size*seq_length, NumBits))\n",
    "            train.accuracy = 0.99*train.accuracy + 0.01*accuracy\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                accuacy = accuracySDR(output, targets.view(batch_size*seq_length, NumBits))\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = multi_hot_encoder(x, n_chars)\n",
    "                    y = multi_hot_encoder(y, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length, NumBits))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"SDR Acc: {:.3f}\".format(accuracy))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "AS5Ik3cvewpa",
    "outputId": "43630bc3-427f-46b9-f160-7ade6330efff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(512, 1024, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=1024\n",
    "n_layers=4\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HzJPMVdJewpe",
    "outputId": "135bf543-60fd-4e22-beb4-798a939d0b6a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500... Step: 10... Loss: 0.0878... Val Loss: 0.0874 SDR Acc: 0.155\n",
      "Epoch: 1/500... Step: 20... Loss: 0.0869... Val Loss: 0.0855 SDR Acc: 0.122\n",
      "Epoch: 1/500... Step: 30... Loss: 0.0850... Val Loss: 0.0833 SDR Acc: 0.129\n",
      "Epoch: 1/500... Step: 40... Loss: 0.0834... Val Loss: 0.0819 SDR Acc: 0.136\n",
      "Epoch: 1/500... Step: 50... Loss: 0.0822... Val Loss: 0.0806 SDR Acc: 0.137\n",
      "Epoch: 1/500... Step: 60... Loss: 0.0813... Val Loss: 0.0797 SDR Acc: 0.140\n",
      "Epoch: 2/500... Step: 70... Loss: 0.0806... Val Loss: 0.0791 SDR Acc: 0.142\n",
      "Epoch: 2/500... Step: 80... Loss: 0.0801... Val Loss: 0.0789 SDR Acc: 0.142\n",
      "Epoch: 2/500... Step: 90... Loss: 0.0802... Val Loss: 0.0789 SDR Acc: 0.143\n",
      "Epoch: 2/500... Step: 100... Loss: 0.0801... Val Loss: 0.0789 SDR Acc: 0.145\n",
      "Epoch: 2/500... Step: 110... Loss: 0.0801... Val Loss: 0.0788 SDR Acc: 0.145\n",
      "Epoch: 2/500... Step: 120... Loss: 0.0799... Val Loss: 0.0788 SDR Acc: 0.149\n",
      "Epoch: 3/500... Step: 130... Loss: 0.0799... Val Loss: 0.0788 SDR Acc: 0.148\n",
      "Epoch: 3/500... Step: 140... Loss: 0.0795... Val Loss: 0.0788 SDR Acc: 0.150\n",
      "Epoch: 3/500... Step: 150... Loss: 0.0797... Val Loss: 0.0788 SDR Acc: 0.150\n",
      "Epoch: 3/500... Step: 160... Loss: 0.0797... Val Loss: 0.0788 SDR Acc: 0.152\n",
      "Epoch: 3/500... Step: 170... Loss: 0.0796... Val Loss: 0.0788 SDR Acc: 0.152\n",
      "Epoch: 3/500... Step: 180... Loss: 0.0794... Val Loss: 0.0788 SDR Acc: 0.152\n",
      "Epoch: 4/500... Step: 190... Loss: 0.0793... Val Loss: 0.0787 SDR Acc: 0.154\n",
      "Epoch: 4/500... Step: 200... Loss: 0.0791... Val Loss: 0.0787 SDR Acc: 0.155\n",
      "Epoch: 4/500... Step: 210... Loss: 0.0793... Val Loss: 0.0787 SDR Acc: 0.155\n",
      "Epoch: 4/500... Step: 220... Loss: 0.0792... Val Loss: 0.0788 SDR Acc: 0.157\n",
      "Epoch: 4/500... Step: 230... Loss: 0.0793... Val Loss: 0.0788 SDR Acc: 0.156\n",
      "Epoch: 4/500... Step: 240... Loss: 0.0791... Val Loss: 0.0788 SDR Acc: 0.158\n",
      "Epoch: 5/500... Step: 250... Loss: 0.0791... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 5/500... Step: 260... Loss: 0.0792... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 5/500... Step: 270... Loss: 0.0790... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 5/500... Step: 280... Loss: 0.0792... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 5/500... Step: 290... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 5/500... Step: 300... Loss: 0.0790... Val Loss: 0.0788 SDR Acc: 0.160\n",
      "Epoch: 6/500... Step: 310... Loss: 0.0791... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 6/500... Step: 320... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 6/500... Step: 330... Loss: 0.0789... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 6/500... Step: 340... Loss: 0.0789... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 6/500... Step: 350... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 6/500... Step: 360... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 7/500... Step: 370... Loss: 0.0791... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 7/500... Step: 380... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 7/500... Step: 390... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 7/500... Step: 400... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 7/500... Step: 410... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 7/500... Step: 420... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 8/500... Step: 430... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 8/500... Step: 440... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 8/500... Step: 450... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 8/500... Step: 460... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 8/500... Step: 470... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 8/500... Step: 480... Loss: 0.0789... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 9/500... Step: 490... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 9/500... Step: 500... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.165\n",
      "Epoch: 9/500... Step: 510... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 9/500... Step: 520... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 9/500... Step: 530... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 9/500... Step: 540... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 10/500... Step: 550... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 10/500... Step: 560... Loss: 0.0791... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 10/500... Step: 570... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 10/500... Step: 580... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 10/500... Step: 590... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 10/500... Step: 600... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 10/500... Step: 610... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 11/500... Step: 620... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 11/500... Step: 630... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 11/500... Step: 640... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 11/500... Step: 650... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 11/500... Step: 660... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 11/500... Step: 670... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 12/500... Step: 680... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 12/500... Step: 690... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 12/500... Step: 700... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 12/500... Step: 710... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 12/500... Step: 720... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 12/500... Step: 730... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 13/500... Step: 740... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 13/500... Step: 750... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 13/500... Step: 760... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 13/500... Step: 770... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 13/500... Step: 780... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 13/500... Step: 790... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 14/500... Step: 800... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 14/500... Step: 810... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 14/500... Step: 820... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 14/500... Step: 830... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 14/500... Step: 840... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 14/500... Step: 850... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 15/500... Step: 860... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 15/500... Step: 870... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 15/500... Step: 880... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 15/500... Step: 890... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 15/500... Step: 900... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 15/500... Step: 910... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 16/500... Step: 920... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 16/500... Step: 930... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 16/500... Step: 940... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 16/500... Step: 950... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 16/500... Step: 960... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.164\n",
      "Epoch: 16/500... Step: 970... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 17/500... Step: 980... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 17/500... Step: 990... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 17/500... Step: 1000... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 17/500... Step: 1010... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 17/500... Step: 1020... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 17/500... Step: 1030... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.155\n",
      "Epoch: 18/500... Step: 1040... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 18/500... Step: 1050... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 18/500... Step: 1060... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/500... Step: 1070... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 18/500... Step: 1080... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 18/500... Step: 1090... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 19/500... Step: 1100... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 19/500... Step: 1110... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 19/500... Step: 1120... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 19/500... Step: 1130... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 19/500... Step: 1140... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 19/500... Step: 1150... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 20/500... Step: 1160... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 20/500... Step: 1170... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 20/500... Step: 1180... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 20/500... Step: 1190... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 20/500... Step: 1200... Loss: 0.0780... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 20/500... Step: 1210... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 20/500... Step: 1220... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 21/500... Step: 1230... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 21/500... Step: 1240... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.164\n",
      "Epoch: 21/500... Step: 1250... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 21/500... Step: 1260... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 21/500... Step: 1270... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 21/500... Step: 1280... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 22/500... Step: 1290... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 22/500... Step: 1300... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 22/500... Step: 1310... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 22/500... Step: 1320... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 22/500... Step: 1330... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 22/500... Step: 1340... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 23/500... Step: 1350... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 23/500... Step: 1360... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 23/500... Step: 1370... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 23/500... Step: 1380... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 23/500... Step: 1390... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 23/500... Step: 1400... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 24/500... Step: 1410... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 24/500... Step: 1420... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 24/500... Step: 1430... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 24/500... Step: 1440... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 24/500... Step: 1450... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 24/500... Step: 1460... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 25/500... Step: 1470... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 25/500... Step: 1480... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 25/500... Step: 1490... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 25/500... Step: 1500... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 25/500... Step: 1510... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 25/500... Step: 1520... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 26/500... Step: 1530... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 26/500... Step: 1540... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 26/500... Step: 1550... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 26/500... Step: 1560... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 26/500... Step: 1570... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 26/500... Step: 1580... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 27/500... Step: 1590... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 27/500... Step: 1600... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 27/500... Step: 1610... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 27/500... Step: 1620... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 27/500... Step: 1630... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 27/500... Step: 1640... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 28/500... Step: 1650... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 28/500... Step: 1660... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 28/500... Step: 1670... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 28/500... Step: 1680... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 28/500... Step: 1690... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 28/500... Step: 1700... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 29/500... Step: 1710... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 29/500... Step: 1720... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 29/500... Step: 1730... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 29/500... Step: 1740... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 29/500... Step: 1750... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 29/500... Step: 1760... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 30/500... Step: 1770... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 30/500... Step: 1780... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 30/500... Step: 1790... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 30/500... Step: 1800... Loss: 0.0781... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 30/500... Step: 1810... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.155\n",
      "Epoch: 30/500... Step: 1820... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 30/500... Step: 1830... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 31/500... Step: 1840... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 31/500... Step: 1850... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.165\n",
      "Epoch: 31/500... Step: 1860... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 31/500... Step: 1870... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 31/500... Step: 1880... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 31/500... Step: 1890... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 32/500... Step: 1900... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 32/500... Step: 1910... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 32/500... Step: 1920... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 32/500... Step: 1930... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 32/500... Step: 1940... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 32/500... Step: 1950... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 33/500... Step: 1960... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 33/500... Step: 1970... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 33/500... Step: 1980... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 33/500... Step: 1990... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 33/500... Step: 2000... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 33/500... Step: 2010... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 34/500... Step: 2020... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 34/500... Step: 2030... Loss: 0.0780... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 34/500... Step: 2040... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 34/500... Step: 2050... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 34/500... Step: 2060... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 34/500... Step: 2070... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 35/500... Step: 2080... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 35/500... Step: 2090... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 35/500... Step: 2100... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/500... Step: 2110... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 35/500... Step: 2120... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 35/500... Step: 2130... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 36/500... Step: 2140... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 36/500... Step: 2150... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 36/500... Step: 2160... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 36/500... Step: 2170... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 36/500... Step: 2180... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 36/500... Step: 2190... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 37/500... Step: 2200... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 37/500... Step: 2210... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 37/500... Step: 2220... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 37/500... Step: 2230... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 37/500... Step: 2240... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 37/500... Step: 2250... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 38/500... Step: 2260... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 38/500... Step: 2270... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 38/500... Step: 2280... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 38/500... Step: 2290... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 38/500... Step: 2300... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 38/500... Step: 2310... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 39/500... Step: 2320... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 39/500... Step: 2330... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 39/500... Step: 2340... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 39/500... Step: 2350... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 39/500... Step: 2360... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 39/500... Step: 2370... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 40/500... Step: 2380... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 40/500... Step: 2390... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.155\n",
      "Epoch: 40/500... Step: 2400... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 40/500... Step: 2410... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 40/500... Step: 2420... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 40/500... Step: 2430... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 40/500... Step: 2440... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 41/500... Step: 2450... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 41/500... Step: 2460... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.164\n",
      "Epoch: 41/500... Step: 2470... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 41/500... Step: 2480... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 41/500... Step: 2490... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 41/500... Step: 2500... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 42/500... Step: 2510... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 42/500... Step: 2520... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 42/500... Step: 2530... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 42/500... Step: 2540... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 42/500... Step: 2550... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 42/500... Step: 2560... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 43/500... Step: 2570... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 43/500... Step: 2580... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 43/500... Step: 2590... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.155\n",
      "Epoch: 43/500... Step: 2600... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 43/500... Step: 2610... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 43/500... Step: 2620... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 44/500... Step: 2630... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 44/500... Step: 2640... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 44/500... Step: 2650... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 44/500... Step: 2660... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 44/500... Step: 2670... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 44/500... Step: 2680... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 45/500... Step: 2690... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 45/500... Step: 2700... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 45/500... Step: 2710... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 45/500... Step: 2720... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 45/500... Step: 2730... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 45/500... Step: 2740... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.164\n",
      "Epoch: 46/500... Step: 2750... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 46/500... Step: 2760... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 46/500... Step: 2770... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 46/500... Step: 2780... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 46/500... Step: 2790... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 46/500... Step: 2800... Loss: 0.0781... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 47/500... Step: 2810... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 47/500... Step: 2820... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 47/500... Step: 2830... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 47/500... Step: 2840... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 47/500... Step: 2850... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 47/500... Step: 2860... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 48/500... Step: 2870... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 48/500... Step: 2880... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 48/500... Step: 2890... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 48/500... Step: 2900... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 48/500... Step: 2910... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 48/500... Step: 2920... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 49/500... Step: 2930... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 49/500... Step: 2940... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 49/500... Step: 2950... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 49/500... Step: 2960... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 49/500... Step: 2970... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 49/500... Step: 2980... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 50/500... Step: 2990... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 50/500... Step: 3000... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 50/500... Step: 3010... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 50/500... Step: 3020... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 50/500... Step: 3030... Loss: 0.0781... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 50/500... Step: 3040... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 50/500... Step: 3050... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 51/500... Step: 3060... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 51/500... Step: 3070... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.164\n",
      "Epoch: 51/500... Step: 3080... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 51/500... Step: 3090... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 51/500... Step: 3100... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 51/500... Step: 3110... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 52/500... Step: 3120... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 52/500... Step: 3130... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 52/500... Step: 3140... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52/500... Step: 3150... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 52/500... Step: 3160... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 52/500... Step: 3170... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 53/500... Step: 3180... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 53/500... Step: 3190... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 53/500... Step: 3200... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 53/500... Step: 3210... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 53/500... Step: 3220... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 53/500... Step: 3230... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 54/500... Step: 3240... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 54/500... Step: 3250... Loss: 0.0781... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 54/500... Step: 3260... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 54/500... Step: 3270... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 54/500... Step: 3280... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 54/500... Step: 3290... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 55/500... Step: 3300... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 55/500... Step: 3310... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 55/500... Step: 3320... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 55/500... Step: 3330... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 55/500... Step: 3340... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 55/500... Step: 3350... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 56/500... Step: 3360... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 56/500... Step: 3370... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 56/500... Step: 3380... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 56/500... Step: 3390... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 56/500... Step: 3400... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 56/500... Step: 3410... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 57/500... Step: 3420... Loss: 0.0789... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 57/500... Step: 3430... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 57/500... Step: 3440... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 57/500... Step: 3450... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 57/500... Step: 3460... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 57/500... Step: 3470... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 58/500... Step: 3480... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 58/500... Step: 3490... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 58/500... Step: 3500... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.155\n",
      "Epoch: 58/500... Step: 3510... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 58/500... Step: 3520... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 58/500... Step: 3530... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 59/500... Step: 3540... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 59/500... Step: 3550... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 59/500... Step: 3560... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 59/500... Step: 3570... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 59/500... Step: 3580... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 59/500... Step: 3590... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 60/500... Step: 3600... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 60/500... Step: 3610... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 60/500... Step: 3620... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 60/500... Step: 3630... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 60/500... Step: 3640... Loss: 0.0781... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 60/500... Step: 3650... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 60/500... Step: 3660... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 61/500... Step: 3670... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 61/500... Step: 3680... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 61/500... Step: 3690... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 61/500... Step: 3700... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 61/500... Step: 3710... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 61/500... Step: 3720... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 62/500... Step: 3730... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 62/500... Step: 3740... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 62/500... Step: 3750... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.155\n",
      "Epoch: 62/500... Step: 3760... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 62/500... Step: 3770... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 62/500... Step: 3780... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 63/500... Step: 3790... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 63/500... Step: 3800... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 63/500... Step: 3810... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 63/500... Step: 3820... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 63/500... Step: 3830... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 63/500... Step: 3840... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 64/500... Step: 3850... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 64/500... Step: 3860... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 64/500... Step: 3870... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 64/500... Step: 3880... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 64/500... Step: 3890... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 64/500... Step: 3900... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 65/500... Step: 3910... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 65/500... Step: 3920... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 65/500... Step: 3930... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 65/500... Step: 3940... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 65/500... Step: 3950... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 65/500... Step: 3960... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 66/500... Step: 3970... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 66/500... Step: 3980... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 66/500... Step: 3990... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 66/500... Step: 4000... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 66/500... Step: 4010... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 66/500... Step: 4020... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 67/500... Step: 4030... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 67/500... Step: 4040... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 67/500... Step: 4050... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 67/500... Step: 4060... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 67/500... Step: 4070... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 67/500... Step: 4080... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 68/500... Step: 4090... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 68/500... Step: 4100... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 68/500... Step: 4110... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 68/500... Step: 4120... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 68/500... Step: 4130... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 68/500... Step: 4140... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 69/500... Step: 4150... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 69/500... Step: 4160... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 69/500... Step: 4170... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 69/500... Step: 4180... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/500... Step: 4190... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 69/500... Step: 4200... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 70/500... Step: 4210... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 70/500... Step: 4220... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 70/500... Step: 4230... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 70/500... Step: 4240... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 70/500... Step: 4250... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 70/500... Step: 4260... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 70/500... Step: 4270... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 71/500... Step: 4280... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 71/500... Step: 4290... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 71/500... Step: 4300... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 71/500... Step: 4310... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 71/500... Step: 4320... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 71/500... Step: 4330... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 72/500... Step: 4340... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 72/500... Step: 4350... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 72/500... Step: 4360... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 72/500... Step: 4370... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 72/500... Step: 4380... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 72/500... Step: 4390... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 73/500... Step: 4400... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 73/500... Step: 4410... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 73/500... Step: 4420... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 73/500... Step: 4430... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 73/500... Step: 4440... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.155\n",
      "Epoch: 73/500... Step: 4450... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 74/500... Step: 4460... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 74/500... Step: 4470... Loss: 0.0780... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 74/500... Step: 4480... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 74/500... Step: 4490... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 74/500... Step: 4500... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 74/500... Step: 4510... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 75/500... Step: 4520... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 75/500... Step: 4530... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 75/500... Step: 4540... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 75/500... Step: 4550... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 75/500... Step: 4560... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 75/500... Step: 4570... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 76/500... Step: 4580... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 76/500... Step: 4590... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 76/500... Step: 4600... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 76/500... Step: 4610... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 76/500... Step: 4620... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 76/500... Step: 4630... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 77/500... Step: 4640... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 77/500... Step: 4650... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 77/500... Step: 4660... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 77/500... Step: 4670... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 77/500... Step: 4680... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 77/500... Step: 4690... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.155\n",
      "Epoch: 78/500... Step: 4700... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 78/500... Step: 4710... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 78/500... Step: 4720... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 78/500... Step: 4730... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 78/500... Step: 4740... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 78/500... Step: 4750... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 79/500... Step: 4760... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 79/500... Step: 4770... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.164\n",
      "Epoch: 79/500... Step: 4780... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 79/500... Step: 4790... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 79/500... Step: 4800... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 79/500... Step: 4810... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 80/500... Step: 4820... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 80/500... Step: 4830... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 80/500... Step: 4840... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 80/500... Step: 4850... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 80/500... Step: 4860... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 80/500... Step: 4870... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 80/500... Step: 4880... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 81/500... Step: 4890... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 81/500... Step: 4900... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 81/500... Step: 4910... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 81/500... Step: 4920... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 81/500... Step: 4930... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 81/500... Step: 4940... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 82/500... Step: 4950... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 82/500... Step: 4960... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 82/500... Step: 4970... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 82/500... Step: 4980... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 82/500... Step: 4990... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 82/500... Step: 5000... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 83/500... Step: 5010... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 83/500... Step: 5020... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 83/500... Step: 5030... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 83/500... Step: 5040... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 83/500... Step: 5050... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 83/500... Step: 5060... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 84/500... Step: 5070... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 84/500... Step: 5080... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 84/500... Step: 5090... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 84/500... Step: 5100... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 84/500... Step: 5110... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 84/500... Step: 5120... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 85/500... Step: 5130... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 85/500... Step: 5140... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 85/500... Step: 5150... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 85/500... Step: 5160... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 85/500... Step: 5170... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 85/500... Step: 5180... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 86/500... Step: 5190... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 86/500... Step: 5200... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 86/500... Step: 5210... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 86/500... Step: 5220... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/500... Step: 5230... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 86/500... Step: 5240... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 87/500... Step: 5250... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 87/500... Step: 5260... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.164\n",
      "Epoch: 87/500... Step: 5270... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 87/500... Step: 5280... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 87/500... Step: 5290... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 87/500... Step: 5300... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 88/500... Step: 5310... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 88/500... Step: 5320... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 88/500... Step: 5330... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 88/500... Step: 5340... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 88/500... Step: 5350... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 88/500... Step: 5360... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 89/500... Step: 5370... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 89/500... Step: 5380... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 89/500... Step: 5390... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 89/500... Step: 5400... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 89/500... Step: 5410... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 89/500... Step: 5420... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 90/500... Step: 5430... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 90/500... Step: 5440... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 90/500... Step: 5450... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 90/500... Step: 5460... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 90/500... Step: 5470... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 90/500... Step: 5480... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 90/500... Step: 5490... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 91/500... Step: 5500... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 91/500... Step: 5510... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 91/500... Step: 5520... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 91/500... Step: 5530... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 91/500... Step: 5540... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 91/500... Step: 5550... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 92/500... Step: 5560... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 92/500... Step: 5570... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 92/500... Step: 5580... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 92/500... Step: 5590... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 92/500... Step: 5600... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 92/500... Step: 5610... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.155\n",
      "Epoch: 93/500... Step: 5620... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 93/500... Step: 5630... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 93/500... Step: 5640... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 93/500... Step: 5650... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 93/500... Step: 5660... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 93/500... Step: 5670... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 94/500... Step: 5680... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 94/500... Step: 5690... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 94/500... Step: 5700... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 94/500... Step: 5710... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 94/500... Step: 5720... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 94/500... Step: 5730... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 95/500... Step: 5740... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 95/500... Step: 5750... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 95/500... Step: 5760... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 95/500... Step: 5770... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 95/500... Step: 5780... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 95/500... Step: 5790... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 96/500... Step: 5800... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 96/500... Step: 5810... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 96/500... Step: 5820... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 96/500... Step: 5830... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 96/500... Step: 5840... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 96/500... Step: 5850... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 97/500... Step: 5860... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 97/500... Step: 5870... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 97/500... Step: 5880... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 97/500... Step: 5890... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 97/500... Step: 5900... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 97/500... Step: 5910... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 98/500... Step: 5920... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 98/500... Step: 5930... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 98/500... Step: 5940... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 98/500... Step: 5950... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 98/500... Step: 5960... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 98/500... Step: 5970... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 99/500... Step: 5980... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 99/500... Step: 5990... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 99/500... Step: 6000... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 99/500... Step: 6010... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 99/500... Step: 6020... Loss: 0.0782... Val Loss: 0.0788 SDR Acc: 0.159\n",
      "Epoch: 99/500... Step: 6030... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 100/500... Step: 6040... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 100/500... Step: 6050... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 100/500... Step: 6060... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 100/500... Step: 6070... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 100/500... Step: 6080... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 100/500... Step: 6090... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 100/500... Step: 6100... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 101/500... Step: 6110... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 101/500... Step: 6120... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 101/500... Step: 6130... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 101/500... Step: 6140... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 101/500... Step: 6150... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 101/500... Step: 6160... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.164\n",
      "Epoch: 102/500... Step: 6170... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 102/500... Step: 6180... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 102/500... Step: 6190... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 102/500... Step: 6200... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 102/500... Step: 6210... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 102/500... Step: 6220... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 103/500... Step: 6230... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 103/500... Step: 6240... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 103/500... Step: 6250... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 103/500... Step: 6260... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103/500... Step: 6270... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 103/500... Step: 6280... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 104/500... Step: 6290... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 104/500... Step: 6300... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 104/500... Step: 6310... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 104/500... Step: 6320... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 104/500... Step: 6330... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 104/500... Step: 6340... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 105/500... Step: 6350... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 105/500... Step: 6360... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 105/500... Step: 6370... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 105/500... Step: 6380... Loss: 0.0787... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 105/500... Step: 6390... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 105/500... Step: 6400... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.164\n",
      "Epoch: 106/500... Step: 6410... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 106/500... Step: 6420... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 106/500... Step: 6430... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 106/500... Step: 6440... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 106/500... Step: 6450... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 106/500... Step: 6460... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 107/500... Step: 6470... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.164\n",
      "Epoch: 107/500... Step: 6480... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 107/500... Step: 6490... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 107/500... Step: 6500... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 107/500... Step: 6510... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 107/500... Step: 6520... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 108/500... Step: 6530... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 108/500... Step: 6540... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 108/500... Step: 6550... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.154\n",
      "Epoch: 108/500... Step: 6560... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 108/500... Step: 6570... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 108/500... Step: 6580... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 109/500... Step: 6590... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 109/500... Step: 6600... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 109/500... Step: 6610... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 109/500... Step: 6620... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 109/500... Step: 6630... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 109/500... Step: 6640... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 110/500... Step: 6650... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 110/500... Step: 6660... Loss: 0.0788... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 110/500... Step: 6670... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 110/500... Step: 6680... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 110/500... Step: 6690... Loss: 0.0781... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 110/500... Step: 6700... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 110/500... Step: 6710... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 111/500... Step: 6720... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.163\n",
      "Epoch: 111/500... Step: 6730... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 111/500... Step: 6740... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 111/500... Step: 6750... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 111/500... Step: 6760... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 111/500... Step: 6770... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 112/500... Step: 6780... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 112/500... Step: 6790... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 112/500... Step: 6800... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 112/500... Step: 6810... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 112/500... Step: 6820... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 112/500... Step: 6830... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 113/500... Step: 6840... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 113/500... Step: 6850... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 113/500... Step: 6860... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 113/500... Step: 6870... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 113/500... Step: 6880... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 113/500... Step: 6890... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 114/500... Step: 6900... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 114/500... Step: 6910... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 114/500... Step: 6920... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 114/500... Step: 6930... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 114/500... Step: 6940... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 114/500... Step: 6950... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 115/500... Step: 6960... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 115/500... Step: 6970... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 115/500... Step: 6980... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 115/500... Step: 6990... Loss: 0.0787... Val Loss: 0.0786 SDR Acc: 0.157\n",
      "Epoch: 115/500... Step: 7000... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 115/500... Step: 7010... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 116/500... Step: 7020... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 116/500... Step: 7030... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 116/500... Step: 7040... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 116/500... Step: 7050... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 116/500... Step: 7060... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 116/500... Step: 7070... Loss: 0.0782... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 117/500... Step: 7080... Loss: 0.0789... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 117/500... Step: 7090... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.161\n",
      "Epoch: 117/500... Step: 7100... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 117/500... Step: 7110... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 117/500... Step: 7120... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 117/500... Step: 7130... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.155\n",
      "Epoch: 118/500... Step: 7140... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 118/500... Step: 7150... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 118/500... Step: 7160... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 118/500... Step: 7170... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 118/500... Step: 7180... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 118/500... Step: 7190... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.156\n",
      "Epoch: 119/500... Step: 7200... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 119/500... Step: 7210... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.163\n",
      "Epoch: 119/500... Step: 7220... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 119/500... Step: 7230... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 119/500... Step: 7240... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 119/500... Step: 7250... Loss: 0.0783... Val Loss: 0.0787 SDR Acc: 0.158\n",
      "Epoch: 120/500... Step: 7260... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 120/500... Step: 7270... Loss: 0.0788... Val Loss: 0.0786 SDR Acc: 0.156\n",
      "Epoch: 120/500... Step: 7280... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 120/500... Step: 7290... Loss: 0.0782... Val Loss: 0.0786 SDR Acc: 0.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120/500... Step: 7300... Loss: 0.0780... Val Loss: 0.0787 SDR Acc: 0.157\n",
      "Epoch: 120/500... Step: 7310... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 120/500... Step: 7320... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 121/500... Step: 7330... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 121/500... Step: 7340... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 121/500... Step: 7350... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 121/500... Step: 7360... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 121/500... Step: 7370... Loss: 0.0784... Val Loss: 0.0786 SDR Acc: 0.162\n",
      "Epoch: 121/500... Step: 7380... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.161\n",
      "Epoch: 122/500... Step: 7390... Loss: 0.0786... Val Loss: 0.0787 SDR Acc: 0.159\n",
      "Epoch: 122/500... Step: 7400... Loss: 0.0783... Val Loss: 0.0786 SDR Acc: 0.158\n",
      "Epoch: 122/500... Step: 7410... Loss: 0.0785... Val Loss: 0.0786 SDR Acc: 0.159\n",
      "Epoch: 122/500... Step: 7420... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n",
      "Epoch: 122/500... Step: 7430... Loss: 0.0786... Val Loss: 0.0786 SDR Acc: 0.160\n",
      "Epoch: 122/500... Step: 7440... Loss: 0.0784... Val Loss: 0.0787 SDR Acc: 0.162\n",
      "Epoch: 123/500... Step: 7450... Loss: 0.0785... Val Loss: 0.0787 SDR Acc: 0.160\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = 160 #max length verses\n",
    "n_epochs = 500 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train.accuracy = 0 \n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 2, 16)\n",
    "x, y = next(batches)\n",
    "\n",
    "x = multi_hot_encoder(x, NumBits)\n",
    "y = multi_hot_encoder(y, NumBits)\n",
    "\n",
    "inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "if(train_on_gpu):\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    \n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "output, h = net(inputs, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = targets.view(batch_size*seq_length, NumBits)\n",
    "# a = a.cpu()\n",
    "# print(np.argwhere(a>0))\n",
    "b = output.cpu()\n",
    "values, indices = b.topk(NumOnBits, dim=1)\n",
    "print(indices.shape)\n",
    "print(indices)\n",
    "print(np.argwhere(b>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbSGCJO4ewpi"
   },
   "outputs": [],
   "source": [
    "model_dante = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_dante, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65NVMnmmewpm"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encoder(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        # apply softmax to get p probabilities for the likely next character giving x\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        # considering the k most probable characters with topk method\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cQz9pJnewps"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='Il', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "ko1gyKMIewpv",
    "outputId": "2c1356c1-fde5-4932-f3b3-128f73459aaa"
   },
   "outputs": [],
   "source": [
    "print(sample(net, 1000, prime='This ', top_k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH5K12reewpy",
    "outputId": "98ba7d8e-f290-407b-86eb-490264eb4cbb"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23n3y1TOewp1",
    "outputId": "5dbde076-cb3b-4593-c3a4-c2e52c22f279"
   },
   "outputs": [],
   "source": [
    "x, y = next(batches)\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7MjK0kEewp5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Char-LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}