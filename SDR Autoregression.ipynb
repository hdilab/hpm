{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Char-LSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdilab/hpm/blob/master/SDR%20Autoregression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR0MzkS3ewoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vawrva3tf9qd",
        "colab_type": "code",
        "outputId": "0bdd8846-26f2-4302-9d6d-e9edef4b674b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwkZG2HSewoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Colab/data/1342.txt','r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMwUVQ4aewoe",
        "colab_type": "code",
        "outputId": "f17718fb-04ad-4cb1-b2bd-df7acbe54fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\\n\\nThis eBook is for the use of any'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "357wL-v5ewoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {c:i for i, c in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDQ9Mcvjewon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(arr, n_labels):\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1. \n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFxKuXihewor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_seq = np.array([[3,5,1]])\n",
        "one_hot=one_hot_encoder(test_seq, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGG5XMZQewov",
        "colab_type": "code",
        "outputId": "f82b60dc-a223-4dbc-d757-74749f69ae78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(test_seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3 5 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4yc-2i9ewoz",
        "colab_type": "code",
        "outputId": "9c436133-58e5-4d23-df2e-a18ce7649484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_seq.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zmNspPrewo5",
        "colab_type": "code",
        "outputId": "81be1056-104f-49a6-cbef-71e7c27f6340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(one_hot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuoOhevSewo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x) \n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:,1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:,1:], arr[:,0] \n",
        "        yield x, y \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeUK4w9ZewpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h42OA4H6ewpG",
        "colab_type": "code",
        "outputId": "00fc1014-24ba-4e57-8abb-d0d19e0ea8ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjxsXcfTewpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=612, n_layers=4, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        r_output, hidden = self.lstm(x,hidden)\n",
        "        \n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flrh6R7-ewpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encoder(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encoder(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS5Ik3cvewpa",
        "colab_type": "code",
        "outputId": "43630bc3-427f-46b9-f160-7ade6330efff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=4\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(84, 512, num_layers=4, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=84, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzJPMVdJewpe",
        "colab_type": "code",
        "outputId": "135bf543-60fd-4e22-beb4-798a939d0b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 64\n",
        "seq_length = 160 #max length verses\n",
        "n_epochs = 50 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 3.2110... Val Loss: 3.1782\n",
            "Epoch: 1/50... Step: 20... Loss: 3.1441... Val Loss: 3.1502\n",
            "Epoch: 1/50... Step: 30... Loss: 3.1455... Val Loss: 3.1451\n",
            "Epoch: 1/50... Step: 40... Loss: 3.1198... Val Loss: 3.1451\n",
            "Epoch: 1/50... Step: 50... Loss: 3.1198... Val Loss: 3.1430\n",
            "Epoch: 1/50... Step: 60... Loss: 3.1236... Val Loss: 3.1461\n",
            "Epoch: 2/50... Step: 70... Loss: 3.1221... Val Loss: 3.1430\n",
            "Epoch: 2/50... Step: 80... Loss: 3.0902... Val Loss: 3.1443\n",
            "Epoch: 2/50... Step: 90... Loss: 3.1087... Val Loss: 3.1428\n",
            "Epoch: 2/50... Step: 100... Loss: 3.1054... Val Loss: 3.1434\n",
            "Epoch: 2/50... Step: 110... Loss: 3.1110... Val Loss: 3.1436\n",
            "Epoch: 2/50... Step: 120... Loss: 3.1024... Val Loss: 3.1458\n",
            "Epoch: 3/50... Step: 130... Loss: 3.1075... Val Loss: 3.1435\n",
            "Epoch: 3/50... Step: 140... Loss: 3.0891... Val Loss: 3.1444\n",
            "Epoch: 3/50... Step: 150... Loss: 3.1031... Val Loss: 3.1430\n",
            "Epoch: 3/50... Step: 160... Loss: 3.1037... Val Loss: 3.1439\n",
            "Epoch: 3/50... Step: 170... Loss: 3.1043... Val Loss: 3.1447\n",
            "Epoch: 3/50... Step: 180... Loss: 3.1009... Val Loss: 3.1463\n",
            "Epoch: 4/50... Step: 190... Loss: 3.0904... Val Loss: 3.1437\n",
            "Epoch: 4/50... Step: 200... Loss: 3.0796... Val Loss: 3.1444\n",
            "Epoch: 4/50... Step: 210... Loss: 3.1022... Val Loss: 3.1441\n",
            "Epoch: 4/50... Step: 220... Loss: 3.0972... Val Loss: 3.1447\n",
            "Epoch: 4/50... Step: 230... Loss: 3.1076... Val Loss: 3.1463\n",
            "Epoch: 4/50... Step: 240... Loss: 3.0885... Val Loss: 3.1455\n",
            "Epoch: 5/50... Step: 250... Loss: 3.0913... Val Loss: 3.1442\n",
            "Epoch: 5/50... Step: 260... Loss: 3.1036... Val Loss: 3.1442\n",
            "Epoch: 5/50... Step: 270... Loss: 3.1008... Val Loss: 3.1445\n",
            "Epoch: 5/50... Step: 280... Loss: 3.1149... Val Loss: 3.1453\n",
            "Epoch: 5/50... Step: 290... Loss: 3.0912... Val Loss: 3.1472\n",
            "Epoch: 5/50... Step: 300... Loss: 3.1034... Val Loss: 3.1447\n",
            "Epoch: 6/50... Step: 310... Loss: 3.1208... Val Loss: 3.1448\n",
            "Epoch: 6/50... Step: 320... Loss: 3.0828... Val Loss: 3.1439\n",
            "Epoch: 6/50... Step: 330... Loss: 3.0964... Val Loss: 3.1449\n",
            "Epoch: 6/50... Step: 340... Loss: 3.1020... Val Loss: 3.1459\n",
            "Epoch: 6/50... Step: 350... Loss: 3.0841... Val Loss: 3.1477\n",
            "Epoch: 6/50... Step: 360... Loss: 3.0807... Val Loss: 3.1442\n",
            "Epoch: 7/50... Step: 370... Loss: 3.1378... Val Loss: 3.1456\n",
            "Epoch: 7/50... Step: 380... Loss: 3.0926... Val Loss: 3.1436\n",
            "Epoch: 7/50... Step: 390... Loss: 3.0920... Val Loss: 3.1453\n",
            "Epoch: 7/50... Step: 400... Loss: 3.0769... Val Loss: 3.1457\n",
            "Epoch: 7/50... Step: 410... Loss: 3.0943... Val Loss: 3.1472\n",
            "Epoch: 7/50... Step: 420... Loss: 3.0985... Val Loss: 3.1436\n",
            "Epoch: 8/50... Step: 430... Loss: 3.1238... Val Loss: 3.1462\n",
            "Epoch: 8/50... Step: 440... Loss: 3.1024... Val Loss: 3.1437\n",
            "Epoch: 8/50... Step: 450... Loss: 3.1034... Val Loss: 3.1458\n",
            "Epoch: 8/50... Step: 460... Loss: 3.0849... Val Loss: 3.1449\n",
            "Epoch: 8/50... Step: 470... Loss: 3.0927... Val Loss: 3.1465\n",
            "Epoch: 8/50... Step: 480... Loss: 3.1106... Val Loss: 3.1434\n",
            "Epoch: 9/50... Step: 490... Loss: 3.1024... Val Loss: 3.1466\n",
            "Epoch: 9/50... Step: 500... Loss: 3.0869... Val Loss: 3.1441\n",
            "Epoch: 9/50... Step: 510... Loss: 3.0798... Val Loss: 3.1461\n",
            "Epoch: 9/50... Step: 520... Loss: 3.0974... Val Loss: 3.1444\n",
            "Epoch: 9/50... Step: 530... Loss: 3.0764... Val Loss: 3.1461\n",
            "Epoch: 9/50... Step: 540... Loss: 3.0959... Val Loss: 3.1433\n",
            "Epoch: 10/50... Step: 550... Loss: 3.0941... Val Loss: 3.1440\n",
            "Epoch: 10/50... Step: 560... Loss: 3.1117... Val Loss: 3.1263\n",
            "Epoch: 10/50... Step: 570... Loss: 3.0529... Val Loss: 3.0700\n",
            "Epoch: 10/50... Step: 580... Loss: 2.9682... Val Loss: 3.0150\n",
            "Epoch: 10/50... Step: 590... Loss: 2.8865... Val Loss: 2.9260\n",
            "Epoch: 10/50... Step: 600... Loss: 2.8600... Val Loss: 2.8678\n",
            "Epoch: 10/50... Step: 610... Loss: 2.7638... Val Loss: 2.7924\n",
            "Epoch: 11/50... Step: 620... Loss: 2.7032... Val Loss: 2.7330\n",
            "Epoch: 11/50... Step: 630... Loss: 2.6682... Val Loss: 2.6889\n",
            "Epoch: 11/50... Step: 640... Loss: 2.6240... Val Loss: 2.6543\n",
            "Epoch: 11/50... Step: 650... Loss: 2.5594... Val Loss: 2.6300\n",
            "Epoch: 11/50... Step: 660... Loss: 2.5416... Val Loss: 2.6062\n",
            "Epoch: 11/50... Step: 670... Loss: 2.5152... Val Loss: 2.5716\n",
            "Epoch: 12/50... Step: 680... Loss: 2.5046... Val Loss: 2.5442\n",
            "Epoch: 12/50... Step: 690... Loss: 2.4400... Val Loss: 2.5246\n",
            "Epoch: 12/50... Step: 700... Loss: 2.4442... Val Loss: 2.4970\n",
            "Epoch: 12/50... Step: 710... Loss: 2.4320... Val Loss: 2.4704\n",
            "Epoch: 12/50... Step: 720... Loss: 2.4243... Val Loss: 2.4638\n",
            "Epoch: 12/50... Step: 730... Loss: 2.3770... Val Loss: 2.4407\n",
            "Epoch: 13/50... Step: 740... Loss: 2.3971... Val Loss: 2.4177\n",
            "Epoch: 13/50... Step: 750... Loss: 2.3538... Val Loss: 2.4003\n",
            "Epoch: 13/50... Step: 760... Loss: 2.3259... Val Loss: 2.3782\n",
            "Epoch: 13/50... Step: 770... Loss: 2.3192... Val Loss: 2.3637\n",
            "Epoch: 13/50... Step: 780... Loss: 2.3119... Val Loss: 2.3561\n",
            "Epoch: 13/50... Step: 790... Loss: 2.2794... Val Loss: 2.3344\n",
            "Epoch: 14/50... Step: 800... Loss: 2.2753... Val Loss: 2.3206\n",
            "Epoch: 14/50... Step: 810... Loss: 2.2432... Val Loss: 2.2979\n",
            "Epoch: 14/50... Step: 820... Loss: 2.2654... Val Loss: 2.2860\n",
            "Epoch: 14/50... Step: 830... Loss: 2.2475... Val Loss: 2.2865\n",
            "Epoch: 14/50... Step: 840... Loss: 2.2433... Val Loss: 2.2670\n",
            "Epoch: 14/50... Step: 850... Loss: 2.1910... Val Loss: 2.2497\n",
            "Epoch: 15/50... Step: 860... Loss: 2.2140... Val Loss: 2.2529\n",
            "Epoch: 15/50... Step: 870... Loss: 2.1942... Val Loss: 2.2336\n",
            "Epoch: 15/50... Step: 880... Loss: 2.1925... Val Loss: 2.2156\n",
            "Epoch: 15/50... Step: 890... Loss: 2.1858... Val Loss: 2.2056\n",
            "Epoch: 15/50... Step: 900... Loss: 2.1436... Val Loss: 2.2000\n",
            "Epoch: 15/50... Step: 910... Loss: 2.1802... Val Loss: 2.1930\n",
            "Epoch: 16/50... Step: 920... Loss: 2.1683... Val Loss: 2.1805\n",
            "Epoch: 16/50... Step: 930... Loss: 2.0834... Val Loss: 2.1617\n",
            "Epoch: 16/50... Step: 940... Loss: 2.1037... Val Loss: 2.1536\n",
            "Epoch: 16/50... Step: 950... Loss: 2.1114... Val Loss: 2.1467\n",
            "Epoch: 16/50... Step: 960... Loss: 2.1071... Val Loss: 2.1376\n",
            "Epoch: 16/50... Step: 970... Loss: 2.0961... Val Loss: 2.1237\n",
            "Epoch: 17/50... Step: 980... Loss: 2.1421... Val Loss: 2.1149\n",
            "Epoch: 17/50... Step: 990... Loss: 2.0362... Val Loss: 2.1000\n",
            "Epoch: 17/50... Step: 1000... Loss: 2.0732... Val Loss: 2.0949\n",
            "Epoch: 17/50... Step: 1010... Loss: 2.0330... Val Loss: 2.0820\n",
            "Epoch: 17/50... Step: 1020... Loss: 2.0253... Val Loss: 2.0727\n",
            "Epoch: 17/50... Step: 1030... Loss: 2.0366... Val Loss: 2.0702\n",
            "Epoch: 18/50... Step: 1040... Loss: 2.0473... Val Loss: 2.0574\n",
            "Epoch: 18/50... Step: 1050... Loss: 1.9939... Val Loss: 2.0414\n",
            "Epoch: 18/50... Step: 1060... Loss: 2.0191... Val Loss: 2.0327\n",
            "Epoch: 18/50... Step: 1070... Loss: 1.9878... Val Loss: 2.0262\n",
            "Epoch: 18/50... Step: 1080... Loss: 1.9480... Val Loss: 2.0181\n",
            "Epoch: 18/50... Step: 1090... Loss: 1.9904... Val Loss: 2.0091\n",
            "Epoch: 19/50... Step: 1100... Loss: 1.9930... Val Loss: 1.9986\n",
            "Epoch: 19/50... Step: 1110... Loss: 1.9255... Val Loss: 1.9868\n",
            "Epoch: 19/50... Step: 1120... Loss: 1.9207... Val Loss: 1.9739\n",
            "Epoch: 19/50... Step: 1130... Loss: 1.9350... Val Loss: 1.9639\n",
            "Epoch: 19/50... Step: 1140... Loss: 1.9146... Val Loss: 1.9643\n",
            "Epoch: 19/50... Step: 1150... Loss: 1.9267... Val Loss: 1.9514\n",
            "Epoch: 20/50... Step: 1160... Loss: 1.9306... Val Loss: 1.9469\n",
            "Epoch: 20/50... Step: 1170... Loss: 1.9235... Val Loss: 1.9288\n",
            "Epoch: 20/50... Step: 1180... Loss: 1.9024... Val Loss: 1.9215\n",
            "Epoch: 20/50... Step: 1190... Loss: 1.8784... Val Loss: 1.9147\n",
            "Epoch: 20/50... Step: 1200... Loss: 1.8465... Val Loss: 1.9066\n",
            "Epoch: 20/50... Step: 1210... Loss: 1.8750... Val Loss: 1.8955\n",
            "Epoch: 20/50... Step: 1220... Loss: 1.8684... Val Loss: 1.8882\n",
            "Epoch: 21/50... Step: 1230... Loss: 1.8441... Val Loss: 1.8807\n",
            "Epoch: 21/50... Step: 1240... Loss: 1.8407... Val Loss: 1.8670\n",
            "Epoch: 21/50... Step: 1250... Loss: 1.8205... Val Loss: 1.8555\n",
            "Epoch: 21/50... Step: 1260... Loss: 1.8104... Val Loss: 1.8509\n",
            "Epoch: 21/50... Step: 1270... Loss: 1.7867... Val Loss: 1.8432\n",
            "Epoch: 21/50... Step: 1280... Loss: 1.7690... Val Loss: 1.8377\n",
            "Epoch: 22/50... Step: 1290... Loss: 1.7986... Val Loss: 1.8287\n",
            "Epoch: 22/50... Step: 1300... Loss: 1.7791... Val Loss: 1.8214\n",
            "Epoch: 22/50... Step: 1310... Loss: 1.7827... Val Loss: 1.8098\n",
            "Epoch: 22/50... Step: 1320... Loss: 1.7537... Val Loss: 1.8034\n",
            "Epoch: 22/50... Step: 1330... Loss: 1.7696... Val Loss: 1.7968\n",
            "Epoch: 22/50... Step: 1340... Loss: 1.7494... Val Loss: 1.7902\n",
            "Epoch: 23/50... Step: 1350... Loss: 1.7693... Val Loss: 1.7837\n",
            "Epoch: 23/50... Step: 1360... Loss: 1.7713... Val Loss: 1.7773\n",
            "Epoch: 23/50... Step: 1370... Loss: 1.7199... Val Loss: 1.7729\n",
            "Epoch: 23/50... Step: 1380... Loss: 1.7148... Val Loss: 1.7668\n",
            "Epoch: 23/50... Step: 1390... Loss: 1.7163... Val Loss: 1.7583\n",
            "Epoch: 23/50... Step: 1400... Loss: 1.7097... Val Loss: 1.7524\n",
            "Epoch: 24/50... Step: 1410... Loss: 1.7053... Val Loss: 1.7465\n",
            "Epoch: 24/50... Step: 1420... Loss: 1.6922... Val Loss: 1.7357\n",
            "Epoch: 24/50... Step: 1430... Loss: 1.6997... Val Loss: 1.7314\n",
            "Epoch: 24/50... Step: 1440... Loss: 1.6707... Val Loss: 1.7280\n",
            "Epoch: 24/50... Step: 1450... Loss: 1.6833... Val Loss: 1.7245\n",
            "Epoch: 24/50... Step: 1460... Loss: 1.6621... Val Loss: 1.7217\n",
            "Epoch: 25/50... Step: 1470... Loss: 1.6663... Val Loss: 1.7135\n",
            "Epoch: 25/50... Step: 1480... Loss: 1.6573... Val Loss: 1.7062\n",
            "Epoch: 25/50... Step: 1490... Loss: 1.6443... Val Loss: 1.7023\n",
            "Epoch: 25/50... Step: 1500... Loss: 1.6877... Val Loss: 1.6988\n",
            "Epoch: 25/50... Step: 1510... Loss: 1.6128... Val Loss: 1.6932\n",
            "Epoch: 25/50... Step: 1520... Loss: 1.6589... Val Loss: 1.6886\n",
            "Epoch: 26/50... Step: 1530... Loss: 1.6617... Val Loss: 1.6834\n",
            "Epoch: 26/50... Step: 1540... Loss: 1.6199... Val Loss: 1.6804\n",
            "Epoch: 26/50... Step: 1550... Loss: 1.6067... Val Loss: 1.6728\n",
            "Epoch: 26/50... Step: 1560... Loss: 1.6084... Val Loss: 1.6725\n",
            "Epoch: 26/50... Step: 1570... Loss: 1.6411... Val Loss: 1.6672\n",
            "Epoch: 26/50... Step: 1580... Loss: 1.6325... Val Loss: 1.6622\n",
            "Epoch: 27/50... Step: 1590... Loss: 1.6640... Val Loss: 1.6566\n",
            "Epoch: 27/50... Step: 1600... Loss: 1.5903... Val Loss: 1.6493\n",
            "Epoch: 27/50... Step: 1610... Loss: 1.6206... Val Loss: 1.6494\n",
            "Epoch: 27/50... Step: 1620... Loss: 1.5993... Val Loss: 1.6478\n",
            "Epoch: 27/50... Step: 1630... Loss: 1.5767... Val Loss: 1.6461\n",
            "Epoch: 27/50... Step: 1640... Loss: 1.5940... Val Loss: 1.6382\n",
            "Epoch: 28/50... Step: 1650... Loss: 1.6175... Val Loss: 1.6300\n",
            "Epoch: 28/50... Step: 1660... Loss: 1.5656... Val Loss: 1.6275\n",
            "Epoch: 28/50... Step: 1670... Loss: 1.5772... Val Loss: 1.6236\n",
            "Epoch: 28/50... Step: 1680... Loss: 1.5843... Val Loss: 1.6219\n",
            "Epoch: 28/50... Step: 1690... Loss: 1.5316... Val Loss: 1.6208\n",
            "Epoch: 28/50... Step: 1700... Loss: 1.5911... Val Loss: 1.6191\n",
            "Epoch: 29/50... Step: 1710... Loss: 1.5813... Val Loss: 1.6112\n",
            "Epoch: 29/50... Step: 1720... Loss: 1.5491... Val Loss: 1.6069\n",
            "Epoch: 29/50... Step: 1730... Loss: 1.5140... Val Loss: 1.6038\n",
            "Epoch: 29/50... Step: 1740... Loss: 1.5456... Val Loss: 1.6034\n",
            "Epoch: 29/50... Step: 1750... Loss: 1.5389... Val Loss: 1.6031\n",
            "Epoch: 29/50... Step: 1760... Loss: 1.5420... Val Loss: 1.5954\n",
            "Epoch: 30/50... Step: 1770... Loss: 1.5692... Val Loss: 1.5959\n",
            "Epoch: 30/50... Step: 1780... Loss: 1.5349... Val Loss: 1.5899\n",
            "Epoch: 30/50... Step: 1790... Loss: 1.5314... Val Loss: 1.5858\n",
            "Epoch: 30/50... Step: 1800... Loss: 1.5237... Val Loss: 1.5860\n",
            "Epoch: 30/50... Step: 1810... Loss: 1.4844... Val Loss: 1.5822\n",
            "Epoch: 30/50... Step: 1820... Loss: 1.5337... Val Loss: 1.5777\n",
            "Epoch: 30/50... Step: 1830... Loss: 1.5208... Val Loss: 1.5785\n",
            "Epoch: 31/50... Step: 1840... Loss: 1.5221... Val Loss: 1.5705\n",
            "Epoch: 31/50... Step: 1850... Loss: 1.5171... Val Loss: 1.5656\n",
            "Epoch: 31/50... Step: 1860... Loss: 1.4987... Val Loss: 1.5672\n",
            "Epoch: 31/50... Step: 1870... Loss: 1.4986... Val Loss: 1.5642\n",
            "Epoch: 31/50... Step: 1880... Loss: 1.4714... Val Loss: 1.5603\n",
            "Epoch: 31/50... Step: 1890... Loss: 1.4702... Val Loss: 1.5597\n",
            "Epoch: 32/50... Step: 1900... Loss: 1.4987... Val Loss: 1.5550\n",
            "Epoch: 32/50... Step: 1910... Loss: 1.4880... Val Loss: 1.5528\n",
            "Epoch: 32/50... Step: 1920... Loss: 1.4862... Val Loss: 1.5500\n",
            "Epoch: 32/50... Step: 1930... Loss: 1.4809... Val Loss: 1.5475\n",
            "Epoch: 32/50... Step: 1940... Loss: 1.4927... Val Loss: 1.5455\n",
            "Epoch: 32/50... Step: 1950... Loss: 1.4861... Val Loss: 1.5461\n",
            "Epoch: 33/50... Step: 1960... Loss: 1.4914... Val Loss: 1.5417\n",
            "Epoch: 33/50... Step: 1970... Loss: 1.5036... Val Loss: 1.5372\n",
            "Epoch: 33/50... Step: 1980... Loss: 1.4650... Val Loss: 1.5350\n",
            "Epoch: 33/50... Step: 1990... Loss: 1.4551... Val Loss: 1.5389\n",
            "Epoch: 33/50... Step: 2000... Loss: 1.4576... Val Loss: 1.5343\n",
            "Epoch: 33/50... Step: 2010... Loss: 1.4687... Val Loss: 1.5320\n",
            "Epoch: 34/50... Step: 2020... Loss: 1.4395... Val Loss: 1.5261\n",
            "Epoch: 34/50... Step: 2030... Loss: 1.4487... Val Loss: 1.5248\n",
            "Epoch: 34/50... Step: 2040... Loss: 1.4375... Val Loss: 1.5187\n",
            "Epoch: 34/50... Step: 2050... Loss: 1.4321... Val Loss: 1.5234\n",
            "Epoch: 34/50... Step: 2060... Loss: 1.4465... Val Loss: 1.5165\n",
            "Epoch: 34/50... Step: 2070... Loss: 1.4290... Val Loss: 1.5195\n",
            "Epoch: 35/50... Step: 2080... Loss: 1.4392... Val Loss: 1.5135\n",
            "Epoch: 35/50... Step: 2090... Loss: 1.4205... Val Loss: 1.5083\n",
            "Epoch: 35/50... Step: 2100... Loss: 1.4178... Val Loss: 1.5082\n",
            "Epoch: 35/50... Step: 2110... Loss: 1.4664... Val Loss: 1.5096\n",
            "Epoch: 35/50... Step: 2120... Loss: 1.4014... Val Loss: 1.5062\n",
            "Epoch: 35/50... Step: 2130... Loss: 1.4404... Val Loss: 1.5071\n",
            "Epoch: 36/50... Step: 2140... Loss: 1.4449... Val Loss: 1.5034\n",
            "Epoch: 36/50... Step: 2150... Loss: 1.4099... Val Loss: 1.4987\n",
            "Epoch: 36/50... Step: 2160... Loss: 1.3918... Val Loss: 1.4983\n",
            "Epoch: 36/50... Step: 2170... Loss: 1.3987... Val Loss: 1.5002\n",
            "Epoch: 36/50... Step: 2180... Loss: 1.4328... Val Loss: 1.4978\n",
            "Epoch: 36/50... Step: 2190... Loss: 1.4195... Val Loss: 1.4953\n",
            "Epoch: 37/50... Step: 2200... Loss: 1.4637... Val Loss: 1.4894\n",
            "Epoch: 37/50... Step: 2210... Loss: 1.3905... Val Loss: 1.4850\n",
            "Epoch: 37/50... Step: 2220... Loss: 1.4309... Val Loss: 1.4860\n",
            "Epoch: 37/50... Step: 2230... Loss: 1.4130... Val Loss: 1.4876\n",
            "Epoch: 37/50... Step: 2240... Loss: 1.3824... Val Loss: 1.4854\n",
            "Epoch: 37/50... Step: 2250... Loss: 1.4143... Val Loss: 1.4824\n",
            "Epoch: 38/50... Step: 2260... Loss: 1.4366... Val Loss: 1.4801\n",
            "Epoch: 38/50... Step: 2270... Loss: 1.3858... Val Loss: 1.4753\n",
            "Epoch: 38/50... Step: 2280... Loss: 1.3948... Val Loss: 1.4758\n",
            "Epoch: 38/50... Step: 2290... Loss: 1.4036... Val Loss: 1.4760\n",
            "Epoch: 38/50... Step: 2300... Loss: 1.3576... Val Loss: 1.4721\n",
            "Epoch: 38/50... Step: 2310... Loss: 1.4305... Val Loss: 1.4717\n",
            "Epoch: 39/50... Step: 2320... Loss: 1.4045... Val Loss: 1.4685\n",
            "Epoch: 39/50... Step: 2330... Loss: 1.3735... Val Loss: 1.4673\n",
            "Epoch: 39/50... Step: 2340... Loss: 1.3638... Val Loss: 1.4668\n",
            "Epoch: 39/50... Step: 2350... Loss: 1.3710... Val Loss: 1.4680\n",
            "Epoch: 39/50... Step: 2360... Loss: 1.3767... Val Loss: 1.4639\n",
            "Epoch: 39/50... Step: 2370... Loss: 1.3780... Val Loss: 1.4624\n",
            "Epoch: 40/50... Step: 2380... Loss: 1.4076... Val Loss: 1.4623\n",
            "Epoch: 40/50... Step: 2390... Loss: 1.3663... Val Loss: 1.4591\n",
            "Epoch: 40/50... Step: 2400... Loss: 1.3814... Val Loss: 1.4567\n",
            "Epoch: 40/50... Step: 2410... Loss: 1.3665... Val Loss: 1.4581\n",
            "Epoch: 40/50... Step: 2420... Loss: 1.3217... Val Loss: 1.4556\n",
            "Epoch: 40/50... Step: 2430... Loss: 1.3748... Val Loss: 1.4551\n",
            "Epoch: 40/50... Step: 2440... Loss: 1.3667... Val Loss: 1.4564\n",
            "Epoch: 41/50... Step: 2450... Loss: 1.3708... Val Loss: 1.4540\n",
            "Epoch: 41/50... Step: 2460... Loss: 1.3597... Val Loss: 1.4530\n",
            "Epoch: 41/50... Step: 2470... Loss: 1.3495... Val Loss: 1.4490\n",
            "Epoch: 41/50... Step: 2480... Loss: 1.3615... Val Loss: 1.4496\n",
            "Epoch: 41/50... Step: 2490... Loss: 1.3400... Val Loss: 1.4492\n",
            "Epoch: 41/50... Step: 2500... Loss: 1.3299... Val Loss: 1.4511\n",
            "Epoch: 42/50... Step: 2510... Loss: 1.3551... Val Loss: 1.4437\n",
            "Epoch: 42/50... Step: 2520... Loss: 1.3577... Val Loss: 1.4404\n",
            "Epoch: 42/50... Step: 2530... Loss: 1.3596... Val Loss: 1.4406\n",
            "Epoch: 42/50... Step: 2540... Loss: 1.3460... Val Loss: 1.4414\n",
            "Epoch: 42/50... Step: 2550... Loss: 1.3509... Val Loss: 1.4398\n",
            "Epoch: 42/50... Step: 2560... Loss: 1.3567... Val Loss: 1.4403\n",
            "Epoch: 43/50... Step: 2570... Loss: 1.3465... Val Loss: 1.4340\n",
            "Epoch: 43/50... Step: 2580... Loss: 1.3664... Val Loss: 1.4308\n",
            "Epoch: 43/50... Step: 2590... Loss: 1.3337... Val Loss: 1.4335\n",
            "Epoch: 43/50... Step: 2600... Loss: 1.3301... Val Loss: 1.4355\n",
            "Epoch: 43/50... Step: 2610... Loss: 1.3365... Val Loss: 1.4345\n",
            "Epoch: 43/50... Step: 2620... Loss: 1.3440... Val Loss: 1.4380\n",
            "Epoch: 44/50... Step: 2630... Loss: 1.3137... Val Loss: 1.4317\n",
            "Epoch: 44/50... Step: 2640... Loss: 1.3250... Val Loss: 1.4266\n",
            "Epoch: 44/50... Step: 2650... Loss: 1.3209... Val Loss: 1.4247\n",
            "Epoch: 44/50... Step: 2660... Loss: 1.3125... Val Loss: 1.4290\n",
            "Epoch: 44/50... Step: 2670... Loss: 1.3312... Val Loss: 1.4291\n",
            "Epoch: 44/50... Step: 2680... Loss: 1.3266... Val Loss: 1.4280\n",
            "Epoch: 45/50... Step: 2690... Loss: 1.3119... Val Loss: 1.4231\n",
            "Epoch: 45/50... Step: 2700... Loss: 1.3016... Val Loss: 1.4209\n",
            "Epoch: 45/50... Step: 2710... Loss: 1.3062... Val Loss: 1.4188\n",
            "Epoch: 45/50... Step: 2720... Loss: 1.3567... Val Loss: 1.4246\n",
            "Epoch: 45/50... Step: 2730... Loss: 1.2855... Val Loss: 1.4212\n",
            "Epoch: 45/50... Step: 2740... Loss: 1.3361... Val Loss: 1.4181\n",
            "Epoch: 46/50... Step: 2750... Loss: 1.3282... Val Loss: 1.4148\n",
            "Epoch: 46/50... Step: 2760... Loss: 1.3081... Val Loss: 1.4184\n",
            "Epoch: 46/50... Step: 2770... Loss: 1.2851... Val Loss: 1.4142\n",
            "Epoch: 46/50... Step: 2780... Loss: 1.2972... Val Loss: 1.4209\n",
            "Epoch: 46/50... Step: 2790... Loss: 1.3228... Val Loss: 1.4150\n",
            "Epoch: 46/50... Step: 2800... Loss: 1.3038... Val Loss: 1.4142\n",
            "Epoch: 47/50... Step: 2810... Loss: 1.3277... Val Loss: 1.4113\n",
            "Epoch: 47/50... Step: 2820... Loss: 1.2968... Val Loss: 1.4156\n",
            "Epoch: 47/50... Step: 2830... Loss: 1.3249... Val Loss: 1.4126\n",
            "Epoch: 47/50... Step: 2840... Loss: 1.3148... Val Loss: 1.4143\n",
            "Epoch: 47/50... Step: 2850... Loss: 1.2780... Val Loss: 1.4122\n",
            "Epoch: 47/50... Step: 2860... Loss: 1.3080... Val Loss: 1.4116\n",
            "Epoch: 48/50... Step: 2870... Loss: 1.3358... Val Loss: 1.4098\n",
            "Epoch: 48/50... Step: 2880... Loss: 1.2874... Val Loss: 1.4068\n",
            "Epoch: 48/50... Step: 2890... Loss: 1.3025... Val Loss: 1.4022\n",
            "Epoch: 48/50... Step: 2900... Loss: 1.3042... Val Loss: 1.4083\n",
            "Epoch: 48/50... Step: 2910... Loss: 1.2694... Val Loss: 1.4059\n",
            "Epoch: 48/50... Step: 2920... Loss: 1.3179... Val Loss: 1.4034\n",
            "Epoch: 49/50... Step: 2930... Loss: 1.3127... Val Loss: 1.4037\n",
            "Epoch: 49/50... Step: 2940... Loss: 1.2905... Val Loss: 1.4009\n",
            "Epoch: 49/50... Step: 2950... Loss: 1.2684... Val Loss: 1.3981\n",
            "Epoch: 49/50... Step: 2960... Loss: 1.2810... Val Loss: 1.4012\n",
            "Epoch: 49/50... Step: 2970... Loss: 1.2924... Val Loss: 1.3982\n",
            "Epoch: 49/50... Step: 2980... Loss: 1.2901... Val Loss: 1.3999\n",
            "Epoch: 50/50... Step: 2990... Loss: 1.3201... Val Loss: 1.4048\n",
            "Epoch: 50/50... Step: 3000... Loss: 1.2796... Val Loss: 1.3999\n",
            "Epoch: 50/50... Step: 3010... Loss: 1.2801... Val Loss: 1.3976\n",
            "Epoch: 50/50... Step: 3020... Loss: 1.2686... Val Loss: 1.3966\n",
            "Epoch: 50/50... Step: 3030... Loss: 1.2415... Val Loss: 1.3960\n",
            "Epoch: 50/50... Step: 3040... Loss: 1.2889... Val Loss: 1.3954\n",
            "Epoch: 50/50... Step: 3050... Loss: 1.2872... Val Loss: 1.3994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbSGCJO4ewpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dante = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_dante, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65NVMnmmewpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encoder(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        # apply softmax to get p probabilities for the likely next character giving x\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        # considering the k most probable characters with topk method\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cQz9pJnewps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='Il', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko1gyKMIewpv",
        "colab_type": "code",
        "outputId": "2c1356c1-fde5-4932-f3b3-128f73459aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "print(sample(net, 1000, prime='This ', top_k=5))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This arain so attentable and his answer;\n",
            "and a months, and a sisters to a she coming anything to\n",
            "astonished her to the lating them as he continued. The\n",
            "was way on, and he taken and to\n",
            "talk to at all off one. It was so\n",
            "left their five\n",
            "will then. He was the person as he said. He had natured. She had been mentioned to an income with a seen\n",
            "her sense that we saw to be in a little of the sancy of the carriage\n",
            "on\n",
            "the were as as to her\n",
            "feelings,\n",
            "and to be in the wife. The part of all her affair of the sort\n",
            "with an an interesting in the cordesion time and\n",
            "the compersed of them a matter of him.\n",
            "\n",
            "What who\n",
            "should have been sorry and\n",
            "concern, he spoke herself on any of Mr. Wickham. The coming out which he had no pleasure over this presence at a man of more of his party of the\n",
            "certain of her so walk, which is not the silent well, and all till they self-tall, which is that\n",
            "he had been series in her seen\n",
            "than\n",
            "her carriage after such a conversation in a consequence, and they had nothing in a little attentio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH5K12reewpy",
        "colab_type": "code",
        "outputId": "98ba7d8e-f290-407b-86eb-490264eb4cbb",
        "colab": {}
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  7,   8,   9,  10,  11,  12],\n",
              "       [ 67,  68,  69,  70,  71,  72],\n",
              "       [127, 128, 129, 130, 131, 132],\n",
              "       [187, 188, 189, 190, 191, 192],\n",
              "       [247, 248, 249, 250, 251, 252]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23n3y1TOewp1",
        "colab_type": "code",
        "outputId": "5dbde076-cb3b-4593-c3a4-c2e52c22f279",
        "colab": {}
      },
      "source": [
        "x, y = next(batches)\n",
        "print(x,y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 54  55  56  57  58  59]\n",
            " [114 115 116 117 118 119]\n",
            " [174 175 176 177 178 179]\n",
            " [234 235 236 237 238 239]\n",
            " [294 295 296 297 298 299]] [[ 55  56  57  58  59   0]\n",
            " [115 116 117 118 119  60]\n",
            " [175 176 177 178 179 120]\n",
            " [235 236 237 238 239 180]\n",
            " [295 296 297 298 299 240]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7MjK0kEewp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}